{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshumer/gpt-oracle-trainer/blob/main/gpt_oracle_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gpt-oracle-trainer\n",
        "Documentation -> fine-tuned LLaMA 2 Q/A system\n",
        "By Matt Shumer (https://twitter.com/mattshumer_)\n",
        "\n",
        "The goal of this notebook is to experiment with a new way to make it very easy to build a task-specific model for answering questions about products and services.\n",
        "\n",
        "To get started:\n",
        "- First, use the best GPU available (go to Runtime -> change runtime type). Add your OpenAI key.\n",
        "\n",
        "- To create your model, just paste in each document in your documentation into the docs list (each as its own string).\n",
        "\n",
        "- Adjust the `service_name_and_description` to match your product/service.\n",
        "\n",
        "- Select a temperature (high=creative, low=precise), and the number of training examples to generate per doc (the more, the better -- ideally do 50+ for this value) to train the model. From there, just run all the cells.\n",
        "\n",
        "You can change the model you want to fine-tune by changing `model_name` in the `Define Hyperparameters` cell."
      ],
      "metadata": {
        "id": "wM8MRkf8Dr94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "zuL2UaqlsmBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = \"OPENAI API KEY HERE\""
      ],
      "metadata": {
        "id": "ARB0zSEVNKZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paste your documentation here. Each file should be a separate string within the `docs` list."
      ],
      "metadata": {
        "id": "FzCSydcq_edi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    \"\"\"Getting Started\n",
        "Let’s get set up with MCLI, the MosaicML command line interface (CLI) and SDK. Install MCLI via pip into your python3 environment:\n",
        "\n",
        "pip install --upgrade mosaicml-cli\n",
        "There are two options listed below for configuring an API key. To manage your account or create new keys, visit the MosaicML console.\n",
        "\n",
        "CLI access\n",
        "This command will walk you through getting started and configuring mcli:\n",
        "\n",
        "mcli init\n",
        "You can also reset your API key through the command line at anytime by running:\n",
        "\n",
        "mcli set api-key <new-value>\n",
        "Access through environment variables\n",
        "The MOSAICML_API_KEY environment variable can also be used to configure access to the MosaicML platform:\n",
        "\n",
        "export MOSAICML_API_KEY=<value>\n",
        "Note that the environment variable takes precedent over the api key set through the CLI\n",
        "\n",
        "Advanced configuration\n",
        "CLI Autocomplete\n",
        "We support autocomplete tab completion in bash and zsh shells through argcomplete. To register tab completion:\n",
        "\n",
        "eval \"$(register-python-argcomplete mcli)\"\n",
        "Depending on your shell configuration, you may see an error, zsh: command not found: compdef. In that case, you need to run two commands before register-python-argcomplete:\n",
        "\n",
        "autoload -Uz compinit\n",
        "compinit\n",
        "eval \"$(register-python-argcomplete mcli)\"\n",
        "Other environment variables\n",
        "Below are all environment variables that can be used to configure MCLI. Most defaults can be left as is:\n",
        "\n",
        "Variable\n",
        "\n",
        "Default\n",
        "\n",
        "Description\n",
        "\n",
        "MCLI_CONFIG_DIR\n",
        "\n",
        "~/.mosaic\n",
        "\n",
        "Folder used to store MCLI configuration file\n",
        "\n",
        "MCLI_CONFIG_PATH\n",
        "\n",
        "~/.mosaic/mcli_config\n",
        "\n",
        "File used to store MCLI configuration file\n",
        "\n",
        "MCLI_TIMEOUT\n",
        "\n",
        "10\n",
        "\n",
        "Timeout (seconds) for queries against the MosaicML API\n",
        "\n",
        "MOSAICML_API_KEY\n",
        "\n",
        "Config set API key override\"\"\",\n",
        "    \"\"\"Set up your environment\n",
        "Setting up the environment for your code to run is easily configurable in the MosaicML platform.\n",
        "\n",
        "Secrets\n",
        "Secrets are credentials or other sensitive information used to configure access to a variety of services. Secrets can enable you to:\n",
        "\n",
        "Access a private docker image\n",
        "\n",
        "Access a private github repo\n",
        "\n",
        "Configure API keys, for example, and API key from Weights and Biases for experiment tracking or from the MosaicML platform to launch runs within runs\n",
        "\n",
        "Access storage: AWS S3, GCP, OCI, Coreweave, Cloudflare\n",
        "\n",
        "All secrets are stored securely in a vault, maintained across your clusters, and added to every run and deployment. Your secrets are never shared with other users.\n",
        "\n",
        "For more information, see the Secrets Page\n",
        "\n",
        "Docker\n",
        "Build a docker image with all the required system packages for your code. Especially for large dependencies, including them in your docker will speed up the run start time. For more information, see the Docker documentation.\n",
        "\n",
        "We maintain a set of public docker images for PyTorch, PyTorch Vision, and Composer on DockerHub.\n",
        "\n",
        "To run with an existing docker image, use the image field:\n",
        "\n",
        "\n",
        "YAML\n",
        "image: mosaicml/composer:latest\n",
        "\n",
        "PYTHON\n",
        "Docker Tags\n",
        "\n",
        "We strongly recommend using a fixed tag instead of latest for docker images to ensure reproducibility. Create and use versioned tag names (e.g. v1.7.0) for your docker images.\n",
        "\n",
        "Private images require setting up Docker Secrets with:\n",
        "\n",
        "mcli create secrets docker\n",
        "Environment Variables\n",
        "Create your own\n",
        "To add non-sensitive environment variables, use the env_variables field in your YAML:\n",
        "\n",
        "name: using-env-variables\n",
        "image: bash\n",
        "env_variables:\n",
        "  - key: FOO\n",
        "    value: 'Hello World!'\n",
        "command: |\n",
        "  echo \"$FOO\"\n",
        "MosaicML Platform Environment Variables\n",
        "We automatically set the following environment variables in your run container.\n",
        "\n",
        "Variable\n",
        "\n",
        "Description\n",
        "\n",
        "MASTER_ADDR\n",
        "\n",
        "The network address of the node with rank 0 in the training job\n",
        "\n",
        "MASTER_PORT\n",
        "\n",
        "The network port of the node with rank 0 in the training job\n",
        "\n",
        "NODE_RANK\n",
        "\n",
        "The rank of the node the container is running on, indexed at zero\n",
        "\n",
        "RUN_NAME\n",
        "\n",
        "The name of your run as seen in the output of mcli get runs\n",
        "\n",
        "COMPOSER_RUN_NAME\n",
        "\n",
        "Identical to RUN_NAME, used by composer\n",
        "\n",
        "WORLD_SIZE\n",
        "\n",
        "The total number of GPUs being used for the training run\n",
        "\n",
        "MOSAICML_PLATFORM\n",
        "\n",
        "true if you are using the MosaicML Platform, used by composer\n",
        "\n",
        "PARAMETERS\n",
        "\n",
        "The path that your run parameters are stored in\n",
        "\n",
        "RESUMPTION_INDEX\n",
        "\n",
        "The index of the number of times your run has resumed, starting at zero\n",
        "\n",
        "NUM_NODES\n",
        "\n",
        "The total number of nodes the run is scheduled on\n",
        "\n",
        "LOCAL_WORLD_SIZE\n",
        "\n",
        "The number of GPUs available to the run on each node\n",
        "\n",
        "Many integrations and secrets will also set environment variables automatically, for instance aws s3 secrets will set AWS_CONFIG_FILE and AWS_SHARED_CREDENTIALS_FILE. Refer to the secret documentation to learn more\"\"\",\n",
        "    \"\"\"Training Quickstart\n",
        "You can easily train your model with the MosaicML platform with just a few simple steps. Before starting, make sure you’ve configured MosaicML access\n",
        "\n",
        "Run “Hello World”\n",
        "To submit your first run, copy the below yaml into a file called ‘hello_world.yaml’.:\n",
        "\n",
        "name: hello-world\n",
        "compute:\n",
        "  gpus: 0\n",
        "image: bash\n",
        "command: |\n",
        "  sleep 2\n",
        "  echo Hello World!\n",
        "Then, run:\n",
        "\n",
        "mcli run -f hello_world.yaml --follow\n",
        "If you see “Hello World!”, congratulations on setting up MCLI!\n",
        "\n",
        "Specifying a cluster\n",
        "\n",
        "If you have access to more than one cluster, the --cluster keyword is a required argument to launch runs. You can find all the clusters you have access to with the following command:\n",
        "\n",
        "mcli get clusters\n",
        "And then specify the cluster in the yaml or through the command line:\n",
        "\n",
        "mcli run -f hello_world.yaml --follow --cluster [your_cluster_name]\"\"\",\n",
        "    \"\"\"Inference Quickstart\n",
        "You can easily deploy your model with the MosaicML platform with just a few simple steps. Before starting, make sure you’ve configured MosaicML access\n",
        "\n",
        "Creating Your First Deployment\n",
        "For this tutorial, we’re going to deploy the MPT-7B Instruct model. To submit your first deployment, copy the below yaml into a file called mpt_instruct_deploy.yaml:\n",
        "\n",
        "name: mpt-7b-instruct\n",
        "compute:\n",
        "  gpus: 1\n",
        "  gpu_type: a100_40gb\n",
        "replicas: 1\n",
        "image: mosaicml/inference:0.0.96\n",
        "command: |\n",
        "  export PYTHONPATH=$PYTHONPATH:/code/examples\n",
        "integrations:\n",
        "  - integration_type: git_repo\n",
        "    git_repo: mosaicml/examples\n",
        "    ssh_clone: false\n",
        "    git_commit: 0b348f765c8ba6b2896a6a0834446bfc4a333811\n",
        "model:\n",
        "  download_parameters:\n",
        "    hf_path: mosaicml/mpt-7b-instruct\n",
        "  model_handler: examples.inference-deployments.mpt.mpt_7b_handler.MPTModelHandler\n",
        "  model_parameters:\n",
        "    model_name: mosaicml/mpt-7b-instruct\n",
        "This yaml tells the MosaicML platform that you are requesting a single a100_40gb gpu and would like to download the mpt-7b-instruct model from the HuggingFace hub. The deployment uses the model handler defined in the Mosaic examples repo here. The model_parameters configure the out-of-the-box model handler provided by the MosaicML platform.\n",
        "\n",
        "Then, run:\n",
        "\n",
        "mcli deploy -f mpt_instruct_deploy.yaml\n",
        "Specifying a cluster\n",
        "\n",
        "If you have access to more than one cluster, you’ll need to specify which cluster to deploy with using --cluster <name>. You can check which clusters you have access to using mcli get clusters\n",
        "\n",
        "After you’ve run the deploy command, you’ll see the following output in your terminal (note that the hash after mpt-7b-instruct- is a unique identifier that we append to the deployment name you provided in your yaml):\n",
        "\n",
        "✔  Deployment mpt-7b-instruct-0t30xo submitted.\n",
        "\n",
        "To see the deployment's status, use:\n",
        "\n",
        "mcli get deployments\n",
        "If you run mcli get deployments, you’ll see the following output:\n",
        "\n",
        "NAME                    USER             CLUSTER  GPU_TYPE   GPU_NUM  CREATED_TIME         STATUS\n",
        "mpt-7b-instruct-0t30xo  user.email.com   r7z13    a100_40gb  1        2023-05-17 07:24 PM  PENDING\n",
        "The mcli get deployments command shows you all the deployments in your organization, so you may see deployments that were not created by you.\n",
        "\n",
        "You can also get more details about a specific deployment by running mcli describe deployment mpt-7b-instruct-0t30xo.\n",
        "\n",
        "Interacting With Your Deployment\n",
        "You’ve created your first deployment, congrats! From here, MCLI has a few convenience commands that make it easier for you to interact with your deployment.\n",
        "\n",
        "First, you may want to check your deployment’s status to see if it’s ready to start serving traffic. You can do that by running the following command:\n",
        "\n",
        "mcli ping mpt-7b-instruct-0t30xo\n",
        "If your deployment is ready, you should see the output:\n",
        "\n",
        "mpt-7b-instruct-0t30xo's status:\n",
        "{'status': 200}\n",
        "where the status is an HTTP status code. If your status code is 200, your deployment is ready to server traffic!\n",
        "\n",
        "Let’s try sending a request to your deployment using the Python SDK:\n",
        "\n",
        "from mcli import predict, get_inference_deployment\n",
        "\n",
        "deployment = get_inference_deployment(\"mpt-7b-instruct-0t30xo\")\n",
        "predict(deployment, {\"inputs\": [\"hello world!\"]})\n",
        "You can also make the same request via the command line:\n",
        "\n",
        "mcli predict mpt-7b-instruct-0t30xo --input '{\"inputs\": [\"hello world!\"]}'\n",
        "You can also do the same with a basic curl command:\n",
        "\n",
        "curl https://mpt-7b-instruct-0t30xo.inf.hosted-on.mosaicml.hosting/predict_stream \\\n",
        "-H \"Authorization: <your_api_key>\" \\\n",
        "-d '{\"inputs\": [\"hello world!\"]}'\n",
        "The address above is for the example, you can look up the address for your own deployment using mcli describe deployment <name>\n",
        "\n",
        "Once you’re done with your deployment, you can delete it with the following command:\n",
        "\n",
        "mcli delete deployments --name mpt-7b-instruct-0t30xo\n",
        "Next Steps\n",
        "There are many more ways you can customize your deployments. We support downloading checkpoint files from any remote storage such as s3 and you can customize your model’s forward logic by implementing a custom model handler. You can even write your own webserver and replace the mosaicml/inference image with your own. Take a look at the Inference Schema Page for more information.\"\"\",\n",
        "    \"\"\"Managing Compute\n",
        "The MosaicML platform configures and manages clusters for you automatically.\n",
        "\n",
        "To view clusters you have access to:\n",
        "\n",
        "mcli get clusters\n",
        "View current cluster utilization:\n",
        "\n",
        "mcli util\n",
        "Requesting compute resources\n",
        "When submitting a run or deployment on a cluster, the MosaicML platform will try and infer which compute resources to use automatically. Which fields are required depend on which and what type of clusters are available to you or your organization. If those resources are not valid or if there are multiple options still available, an error will be raised on run submissions, and the run will not be created.\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "Details\n",
        "\n",
        "gpus\n",
        "\n",
        "int\n",
        "\n",
        "Typically required, unless you specify nodes or a cpu-only run\n",
        "\n",
        "cluster\n",
        "\n",
        "str\n",
        "\n",
        "Required if you have multiple clusters\n",
        "\n",
        "gpu_type\n",
        "\n",
        "str\n",
        "\n",
        "Optional\n",
        "\n",
        "instance\n",
        "\n",
        "str\n",
        "\n",
        "Optional. Only needed if the cluster has multiple GPU instances\n",
        "\n",
        "nodes\n",
        "\n",
        "int\n",
        "\n",
        "Optional. Alternative to gpus - typically there are 8 GPUs per node\n",
        "\n",
        "cpus\n",
        "\n",
        "int\n",
        "\n",
        "Optional\n",
        "\n",
        "For example, you can launch a multi-node cluster my-cluster with 16 A100 GPUs:\n",
        "\n",
        "compute:\n",
        "  cluster: my-cluster\n",
        "  gpus: 16\n",
        "  gpu_type: a100_80gb\n",
        "Most compute fields are also optional CLI arguments\"\"\",\n",
        "    \"\"\"Common Commands\n",
        "mcli run -f <your_yaml>\n",
        "Submits a run with the provided YAML configuration.\n",
        "\n",
        "mcli run --clone <existing_run_name>\n",
        "Submits a new run using the existing run’s configuration\n",
        "\n",
        "mcli get runs\n",
        "Lists all of your submitted runs (see mcli get runs --help to view the many filters available)\n",
        "\n",
        "mcli describe run <run_name>\n",
        "Get detailed information about a run, including the config that was used to launch it.\n",
        "\n",
        "mcli logs <run_name>\n",
        "Retrieves the console log of the latest resumption of the indicated run.\n",
        "\n",
        "mcli logs <run_name> --resumption <N>\n",
        "Retrieves the console log for a given resumption of the indicated run.\n",
        "\n",
        "mcli stop run <run_name>\n",
        "Stops the provided run. The run will be stopped but not deleted from the cluster.\n",
        "\n",
        "mcli run -r <stopped_run>\n",
        "Restarts a stopped run. See Composer’s Auto Resumption guide!\n",
        "\n",
        "mcli delete run <run_name>\n",
        "Deletes the run (and its associated logs) from the cluster.\n",
        "\n",
        "mcli update run <run_name> --max-duration <hours>\n",
        "Updates the max time (in hours) than a run can run for.\n",
        "\n",
        "Full documentation for the mcli update run command\n",
        "Run sharing\n",
        "If run sharing is enabled, users within the same organization have read access to other users’ runs. Ask your administrator if you would like this feature enabled!\n",
        "\n",
        "This enables easier collaboration, so a user can fetch other users’ runs with:\n",
        "\n",
        "mcli get runs --user <another_users_email>\n",
        "Users can also tail the logs and describe another user’s runs with:\n",
        "\n",
        "mcli logs <run_name>\n",
        "Watchdog\n",
        "When training large-scale runs, there may be hardware failures (i.e. node failures). We’ve developed a system called watchdog that will automatically resume your run if our system detects any failures. This is not enabled by default because gracefully resuming models during training requires careful consideration. If you are using Composer or the LLM foundry, this can be easily enabled.\n",
        "\n",
        "You can enable watchdog on an existing and active run.\n",
        "\n",
        "To enable watchdog, use:\n",
        "\n",
        "mcli watchdog <run_name>\n",
        "To disable watchdog, use:\n",
        "\n",
        "mcli watchdog --disable <run_name>\n",
        "If watchdog is enabled for your run, you’ll see a 🐕 icon next to your run_name in the mcli get runs display.\n",
        "\n",
        "By default, enabling watchdog will automatically retry your run 10 times.\n",
        "\n",
        "You can configure this default in your yaml by overriding the max_retries scheduling parameter.\"\"\",\n",
        "    \"\"\"Configure a run\n",
        "Run submissions to the MosaicML platform can be configured through a YAML file or using our Python API’s RunConfig class.\n",
        "\n",
        "The fields are identical across both methods:\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "name\n",
        "\n",
        "required\n",
        "\n",
        "str\n",
        "\n",
        "image\n",
        "\n",
        "required\n",
        "\n",
        "str\n",
        "\n",
        "command\n",
        "\n",
        "required\n",
        "\n",
        "str\n",
        "\n",
        "compute\n",
        "\n",
        "required\n",
        "\n",
        "ComputeConfig\n",
        "\n",
        "scheduling\n",
        "\n",
        "optional\n",
        "\n",
        "SchedulingConfig\n",
        "\n",
        "integrations\n",
        "\n",
        "optional\n",
        "\n",
        "List[Dict]\n",
        "\n",
        "env_variables\n",
        "\n",
        "optional\n",
        "\n",
        "List[Dict]\n",
        "\n",
        "parameters\n",
        "\n",
        "optional\n",
        "\n",
        "Dict[str, Any]\n",
        "\n",
        "metadata\n",
        "\n",
        "optional\n",
        "\n",
        "Dict[str, Any]\n",
        "\n",
        "Here’s an example run configuration:\n",
        "\n",
        "\n",
        "YAML\n",
        "name: hello-composer\n",
        "image: mosaicml/pytorch:latest\n",
        "command: 'echo $MESSAGE'\n",
        "compute:\n",
        "  cluster: <fill-in-with-cluster-name>\n",
        "  gpus: 0\n",
        "scheduling:\n",
        "  priority: low\n",
        "integrations:\n",
        "  - integration_type: git_repo\n",
        "    git_repo: mosaicml/benchmarks\n",
        "    git_branch: main\n",
        "env_variables:\n",
        "  - key: MESSAGE\n",
        "    value: hello composer!\n",
        "\n",
        "PYTHON\n",
        "Setting up YAML Schema in VSCode\n",
        "Autocomplete suggestions and static checking for training YAML files can be supported using by using a JSON Schema in VSCode. Alt text To configure this in your local VSCode environment:\n",
        "\n",
        "Download the YAML extension in VSCode.\n",
        "\n",
        "Download the JSON Schema file training.json here.\n",
        "\n",
        "Go to Settings from Code → Preferences → Settings.\n",
        "\n",
        "Search for YAML Schema and go to Edit in settings.json Alt text\n",
        "\n",
        "Under \"yaml.schemas\", add the code listed below. The key is a link to the JSON file specifying the YAML schema, and the value specifies what the kinds of YAML files that are targetted.\n",
        "\n",
        "\"yaml.schemas\": {\n",
        "        \"https://raw.githubusercontent.com/mosaicml/examples/melissa/yaml_schema/training.json\": \"**/mcli/**/*.yaml\"\n",
        "    }\n",
        "Restart VSCode.\n",
        "\n",
        "Now, CTRL + Space should enable autocomplete for any file located in mcli/.\n",
        "\n",
        "Field Types\n",
        "Run Name\n",
        "A run name is the primary identifier for working with runs. For each run, a unique identifier is automatically appended to the provided run name. After submitting a run, the finalized unique name is displayed in the terminal, and can also be viewed with mcli get runs or Run object.\n",
        "\n",
        "Image\n",
        "Runs are executed within Docker containers defined by a Docker image. Images on DockerHub can be configured as <organization>/<image name>. For private Dockerhub repositories, add a docker secret with:\n",
        "\n",
        "mcli create secret docker\n",
        "For more details, see the Docker Secret Page.\n",
        "\n",
        "Using Alternative Docker Registries\n",
        "\n",
        "While we default to DockerHub, custom registries are supported, see Docker’s documentation and Docker Secret Page for more details.\n",
        "\n",
        "Command\n",
        "The command is what’s executed when the run starts, typically to launch your training jobs and scripts. For example, the following command:\n",
        "\n",
        "command: |\n",
        "  echo Hello World!\n",
        "will result in a run that prints “Hello World” to the console.\n",
        "\n",
        "If you are training models with Composer, then the command field is where you will write your Composer launch command.\n",
        "\n",
        "Compute Fields\n",
        "The compute field specifies which compute resources to request for your run. The MosaicML platform will try and infer which compute resources to use automatically. Which fields are required depend on which and what type of clusters are available to your organization. If those resources are not valid or if there are multiple options still available, an error will be raised on run submissions, and the run will not be created.\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "Details\n",
        "\n",
        "gpus\n",
        "\n",
        "int\n",
        "\n",
        "Typically required, unless you specify nodes or a cpu-only run\n",
        "\n",
        "cluster\n",
        "\n",
        "str\n",
        "\n",
        "Required if you have multiple clusters\n",
        "\n",
        "gpu_type\n",
        "\n",
        "str\n",
        "\n",
        "Optional\n",
        "\n",
        "instance\n",
        "\n",
        "str\n",
        "\n",
        "Optional. Only needed if the cluster has multiple GPU instances\n",
        "\n",
        "nodes\n",
        "\n",
        "int\n",
        "\n",
        "Optional. Alternative to gpus - typically there are 8 GPUs per node\n",
        "\n",
        "cpus\n",
        "\n",
        "int\n",
        "\n",
        "Optional. Typically not used other than for debugging small deployments.\n",
        "\n",
        "You can see clusters, instances, and compute resources available to you using:\n",
        "\n",
        "mcli get clusters\n",
        "For example, you can launch a multi-node cluster my-cluster with 16 A100 GPUs:\n",
        "\n",
        "compute:\n",
        "  cluster: my-cluster\n",
        "  gpus: 16\n",
        "  gpu_type: a100_80gb\n",
        "Scheduling\n",
        "The scheduling field governs how the MosaicML platform’s scheduler will manage your run. It is a simple dictionary, currently containing one key: priority.\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "priority\n",
        "\n",
        "optional\n",
        "\n",
        "str\n",
        "\n",
        "preemptible\n",
        "\n",
        "optional\n",
        "\n",
        "bool\n",
        "\n",
        "max_retries\n",
        "\n",
        "optional\n",
        "\n",
        "int\n",
        "\n",
        "retry_on_system_failure\n",
        "\n",
        "optional\n",
        "\n",
        "bool\n",
        "\n",
        "max_duration_seconds\n",
        "\n",
        "optional\n",
        "\n",
        "int\n",
        "\n",
        "priority: Runs in the platform’s scheduling queue are first sorted by their priority, then by their creation time. The priority field can be one of 3 values: low, default and high. When omitted, the default value is used. Best practices usually dictate that large numbers of more experimental runs (think exploratory hyperparameter sweeps) should usually be run at low priority, whereas important “hero” runs should be run at high priority.\n",
        "\n",
        "preemptible: If your run can be retried, you can set preemptible to True.\n",
        "\n",
        "max_retries: This is the maximum number of times our system will attempt to retry your run.\n",
        "\n",
        "retry_on_system_failure: If you want your run to be retried if it encounters a system failure, you can set retry_on_system_failure to True\n",
        "\n",
        "max_duration_seconds: This is the time duration (in seconds) that your run can run for before it is stopped.\n",
        "\n",
        "Integrations\n",
        "We support many Integrations to customize aspects of both the run setup and environment.\n",
        "\n",
        "Integrations are specified as a list in the YAML. Each item in the list must specify a valid integration_type along with the relevant fields for the requested integration.\n",
        "\n",
        "Some examples of integrations include automatically cloning a Github repository, installing python packages, and setting up logging to a Weights and Biases project are shown below:\n",
        "\n",
        "integrations:\n",
        "  - integration_type: git_repo\n",
        "    git_repo: org/my_repo\n",
        "    git_branch: my-work-branch\n",
        "  - integration_type: pip_packages\n",
        "    packages:\n",
        "      - numpy>=1.22.1\n",
        "      - requests\n",
        "  - integration_type: wandb\n",
        "    project: my_weight_and_biases_project\n",
        "    entity: mosaicml\n",
        "You can read more about integrations on the Integrations Page.\n",
        "\n",
        "Some integrations may require adding secrets. For example, pulling from a private github repository would require the git-ssh secret to be configured. See the Secrets Page.\n",
        "\n",
        "Environment Variables\n",
        "Environment variables can also be injected into each run at runtime through the env_variables field. Each environment variable in the list must have a key and value configured.\n",
        "\n",
        "key: name used to access the value of the environment variable\n",
        "\n",
        "value: value of the environment variable.\n",
        "\n",
        "For example, the below YAML will print “Hello MOSAICML my name is MOSAICML_TWO!”:\n",
        "\n",
        "name: hello-world\n",
        "image: python\n",
        "command: |\n",
        "  sleep 2\n",
        "  echo Hello $NAME my name is $SECOND_NAME!\n",
        "env_variables:\n",
        "  - key: NAME\n",
        "    value: MOSAICML\n",
        "  - key: SECOND_NAME\n",
        "    value: MOSAICML_TWO\n",
        "The command accesses the value of the environment variable by the key field (in this case $NAME and $SECOND_NAME)\n",
        "\n",
        "Parameters\n",
        "The provided parameters are mounted as a YAML file of your run at /mnt/config/parameters.yaml for your code to access. Parameters are a popular way to easily configure your training run.\n",
        "\n",
        "Metadata\n",
        "Metadata is meant to be a multi-purposed, unstructured place to put information about a run. It can be set at the beginning of the run, for example to add custom run-level tags or groupings:\n",
        "\n",
        "name: hello-world\n",
        "image: bash\n",
        "command: echo 'hello world'\n",
        "metadata:\n",
        "  run_type: test\n",
        "Metadata on your run is readable through the CLI or SDK:\n",
        "\n",
        "\n",
        "BASH\n",
        "> mcli describe run hello-world-VC5nFs\n",
        "Run Details\n",
        "Run Name      hello-world-VC5nFs\n",
        "Image         bash\n",
        "...\n",
        "Run Metadata\n",
        "KEY         VALUE\n",
        "run_type    test\n",
        "\n",
        "PYTHON\n",
        "You can also update metadata when the run is running, which can be helpful for exporting metrics or information from the run:\n",
        "\n",
        "from mcli import update_run_metadata\n",
        "\n",
        "run = update_run_metadata(\"hello-world-VC5nFs\", {\"run_type\": \"test_but_updated\"})\n",
        "print(\"New metadata values:\", run.metadata)\n",
        "Metadata size constraints\n",
        "\n",
        "Metadata is not intended for large amounts of data such as time series data. Each key is limited to 200 characters and value is limited to 0.1mb. Metadata cannot have more than 200 keys. A MAPIException will be raised on creation or updates if any of these limits are exceeded.\"\"\",\n",
        "    \"\"\"Run Lifecycle\n",
        "\n",
        "BASH\n",
        "mcli run -f example.yaml\n",
        "\n",
        "PYTHON\n",
        "What happens next? The MosaicML platform manages submitting the run and orchestrates all run requests automatically. The status of a run (RunStatus object) can be monitored using:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli get runs\n",
        "\n",
        "PYTHON\n",
        "\n",
        "CONSOLE\n",
        "This status represents unique phases the run will enter during its lifecycle:\n",
        "\n",
        "\n",
        "\n",
        "Between the “Starting” and “Terminating” phases, your run will be assigned and consuming node resources on the cluster. For this reason, GPU usage is computed as the difference of:\n",
        "\n",
        "Start time: The time the run enters the STARTING status\n",
        "\n",
        "End time: The time the run exits the TERMINATING status\n",
        "\n",
        "Note that runs will never share GPUs, but could be assigned different GPUs on the same node if the cluster supports it.\n",
        "\n",
        "Pending (PENDING)\n",
        "The run has been submitted to the MosaicML platform, but hasn’t been sent to the compute plane or assigned a space in the queue\n",
        "\n",
        "Queued (QUEUED)\n",
        "The run has been placed in queue to be picked up by the specified cluster.\n",
        "\n",
        "If there is space on the cluster, the run will likely appear to skip this phase entirely. In other cases, the run may remain in queued for a long time due to the size of the resource request and/or current cluster utilization. You can view active and queued runs on all clusters using:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli util\n",
        "\n",
        "PYTHON\n",
        "Starting (STARTING)\n",
        "After a run has been scheduled, it goes from the pending to the starting status. In this phase, the scheduler has assigned the run to node(s) in the cluster and has started setting up everything needed to run the workload.\n",
        "\n",
        "Starting a run includes setting up platform-specific containers and pulling the docker image you specified when you configured the run. This phase can be time consuming for large images that have not been used recently in the cluster (caching is done with the Always image pull policy in kubernetes).\n",
        "\n",
        "Running (RUNNING)\n",
        "After finishing the setup, the run goes into the Running phase, which is the core phase where the run is executed inside a containers on one or more nodes.\n",
        "\n",
        "First, any integrations configured for the run are executed, such as cloning a Github repo or installing a pypi package. Integrations are executed in order to produce the required run environment.\n",
        "\n",
        "Once integrations finish building, the command configured for the run is executed.\n",
        "\n",
        "During this phase, you can view the stdout and stderr of any commands run using:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli logs <run>\n",
        "\n",
        "PYTHON\n",
        "\n",
        "CONSOLE\n",
        "Terminating (TERMINATING)\n",
        "After exiting the running or starting phase, runs will always enter a “terminating” status. This phase will typically last up to about 30-40 seconds as the platform kills any remaining processes and removes the run’s assignment to the node(s). Until this phase is complete, the run can still be consuming resources and other runs cannot be scheduled on these node(s).\n",
        "\n",
        "From terminating, there are several terminal phases the run may enter into:\n",
        "\n",
        "Completed: Everything ran smoothly and the run has successfully finished! 🙌\n",
        "\n",
        "Stopped: The run was stopped and is no longer executing\n",
        "\n",
        "Failed: Something went wrong while the run was starting or running\n",
        "\n",
        "Below are details of each terminal phase\n",
        "\n",
        "Completed (COMPLETED)\n",
        "The run has executed the full command and finished without any errors. You now view the full run logs, examine final run metrics, or saved checkpoints and data.\n",
        "\n",
        "If you no longer need this run, you can clean it up using:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli delete run <run>\n",
        "\n",
        "PYTHON\n",
        "\n",
        "CONSOLE\n",
        "Stopped (STOPPED)\n",
        "The run started running but did not complete entirely. This state can be entered by stopping the run using:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli stop run <run>\n",
        "\n",
        "PYTHON\n",
        "\n",
        "CONSOLE\n",
        "A stopped run can then be be restarted using:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli run -r <run>\n",
        "\n",
        "PYTHON\n",
        "\n",
        "CONSOLE\n",
        "When a run restarts, the platform does not automatically save the state of the previous run. Instead, the user code is left responsible for this. If you’re using Composer this is easy to enable through checkpointing and auto-resumption.\n",
        "\n",
        "On restart, the run will begin the run lifecycle again and execute the series of commands from the very beginning. To see all attempts of a run, you can view the entire lifecycle using:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli describe run <run>\n",
        "\n",
        "PYTHON\n",
        "\n",
        "CONSOLE\n",
        "Failed (FAILED)\n",
        "Unfortunately, there are several potential reasons why a run may have failed. This section will go over in depth different failures you may encounter, and how to recover from them.\n",
        "\n",
        "First, make sure you identify that the run has failed and potentially the reason using:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli get run \"run\"\n",
        "\n",
        "PYTHON\n",
        "\n",
        "CONSOLE\n",
        "Below outlines debugging each reason. Take note of the exit code if provided as well\n",
        "\n",
        "Reason: FailedImagePull\n",
        "This means the run failed during the Starting phase when trying to pull the image you’ve specified in the run configuration.\n",
        "\n",
        "There’s a few reasons this could happen:\n",
        "\n",
        "The image is private and docker secrets are not configured or does not have access. To fix this, set up docker secrets and confirm you can pull the image with this combination of username and password\n",
        "\n",
        "The image name is not valid. Double check the image name you entered in the run configuration by describing the run\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli describe run <run>\n",
        "\n",
        "PYTHON\n",
        "Reason: Error\n",
        "This is the catch-all run failure that means something failed when the run was being executed. You’ll want to look at the run logs to debug:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli logs <run>\n",
        "\n",
        "PYTHON\n",
        "The --failed flag will default to showing the logs of the first failed node rank. Note that since runs execute in a unique process for each nodes, the logs for each rank could be different (e.g. one node could have raised an exit code, which would have triggered all other nodes to fail). You can manually specify which node rank to view the logs of using the rank flag:\n",
        "\n",
        "\n",
        "BASH\n",
        "mcli logs <run> --rank 2\"\"\",\n",
        "    \"\"\"Manage a run with the SDK\n",
        "Runs can be managed through the Python API. Below outlines how to work with runs, including creation, following, getting, stopping, and deleting runs. Before getting started, familiarize yourself with Run Lifecycle.\n",
        "\n",
        "Creating a run\n",
        "mcli.api.runs.create_run(run, *, timeout=10, future=False)[source]\n",
        "Launch a run in the MosaicML platform\n",
        "\n",
        "The provided run must contain enough information to fully detail the run\n",
        "\n",
        "PARAMETERS\n",
        "run – A fully-configured run to launch. The run will be queued and persisted in the run database.\n",
        "\n",
        "timeout – Time, in seconds, in which the call should complete. If the run creation takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "future – Return the output as a :type concurrent.futures.Future:. If True, the call to create_run will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the :type Run: output, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "RETURNS\n",
        "A Run that includes the launched run details and the run status\n",
        "\n",
        "Runs can programmatically be created, giving you flexibility to define custom workflows or create similar runs in quick succession. create_run() will takes a RunConfig object, which is a fully-configured run ready to launch. The method will launch the run and then return a Run object, which includes the RunConfig data in Run.config but also data received at the time the run was launched.\n",
        "\n",
        "The RunConfig object\n",
        "The RunConfig object holds configuration data needed to launch a run. This is the underlying python data structure MCLI uses, so before beginning make sure to familiarize yourself with the Run schema.\n",
        "\n",
        "class mcli.api.runs.RunConfig(run_name=None, name=None, gpu_type=None, gpu_num=None, cpus=None, platform=None, cluster=None, image=None, partitions=None, optimization_level=None, integrations=<factory>, env_variables=<factory>, scheduling=<factory>, compute=<factory>, metadata=<factory>, command='', parameters=<factory>, entrypoint='')[source]\n",
        "A run configuration for the MosaicML platform\n",
        "\n",
        "Values in here are not yet validated and some required values may be missing.\n",
        "\n",
        "PARAMETERS\n",
        "name (Optional[str]) – User-defined name of the run\n",
        "\n",
        "gpu_type (Optional[str]) – GPU type (optional if only one gpu type for your cluster)\n",
        "\n",
        "gpu_num (Optional[int]) – Number of GPUs\n",
        "\n",
        "cpus (Optional[int]) – Number of CPUs\n",
        "\n",
        "cluster (Optional[str]) – Cluster to use (optional if you only have one)\n",
        "\n",
        "image (Optional[str]) – Docker image (e.g. mosaicml/composer)\n",
        "\n",
        "integrations (List[Dict[str, Any]]) – List of integrations\n",
        "\n",
        "env_variables (List[Dict[str, str]]) – List of environment variables\n",
        "\n",
        "command (str) – Command to use when a run starts\n",
        "\n",
        "parameters (Dict[str, Any]) – Parameters to mount into the environment\n",
        "\n",
        "entrypoint (str) – Alternative to command\n",
        "\n",
        "There are two ways to initialize a RunConfig object that can be used to config and create a run. The first is by referencing a YAML file, equivalent to the file argument MCLI:\n",
        "\n",
        "from mcli.api.runs import RunConfig, create_run\n",
        "\n",
        "run_config = RunConfig.from_file('hello_world.yaml')\n",
        "created_run = create_run(run_config)\n",
        "Alternatively, you can instantiate the RunConfig object directly in python:\n",
        "\n",
        "from mcli.api.runs import RunConfig, create_run\n",
        "\n",
        "cluster = \"<your-cluster>\"\n",
        "run_config = RunConfig(\n",
        "    name='hello-world',\n",
        "    image='bash',\n",
        "    command='echo \"Hello World!\" && sleep 60',\n",
        "    gpu_type='none',\n",
        "    cluster=cluster,\n",
        ")\n",
        "created_run = create_run(run_config)\n",
        "These can also be used in combination, for example loading a base configuration file and modifying select fields:\n",
        "\n",
        "from mcli.api.runs import RunConfig, create_run\n",
        "\n",
        "special_config = RunConfig.from_file('base_config.yaml')\n",
        "special_config.gpus = 8\n",
        "created_run = create_run(special_config)\n",
        "Changing parameters for parameter sweeps\n",
        "\n",
        "If you are trying to kick off a bunch of runs with similar configurations and different training parameters, make sure you copy the parameters (and any other dict field) instead of modifying them directly\n",
        "\n",
        "import copy\n",
        "\n",
        "config = RunConfig.from_file('base_config.yaml')\n",
        "\n",
        "params = { ... }\n",
        "for lr in (0.1, 0.01, 0.001):\n",
        "    new_params = copy.deepcopy(params)\n",
        "    new_params['optimizers']['sgd']['lr'] = lr\n",
        "    config.parameters = new_params\n",
        "    created_run = create_run(config)\n",
        "The Run object\n",
        "Created runs will be returned as a Run object in create_run(). This object can be used as input to any subsequent run function, for example you can start a run and then immediately start following it:\n",
        "\n",
        "created_run = create_run(config)\n",
        "for line in follow_run_logs(created_run):\n",
        "    print(line)\n",
        "class mcli.api.runs.Run(run_uid, name, status, created_at, updated_at, created_by, priority, preemptible, retry_on_system_failure, cluster, gpus, gpu_type, cpus, node_count, latest_resumption, max_retries=None, reason=None, nodes=<factory>, submitted_config=None, metadata=None, last_resumption_id=None, resumptions=<factory>, lifecycle=<factory>, image=None, _required_properties=('id', 'name', 'status', 'createdAt', 'updatedAt', 'reason', 'createdByEmail', 'priority', 'preemptible', 'retryOnSystemFailure', 'resumptions'))[source]\n",
        "A run that has been launched on the MosaicML platform\n",
        "\n",
        "PARAMETERS\n",
        "run_uid (str) – Unique identifier for the run\n",
        "\n",
        "name (str) – User-defined name of the run\n",
        "\n",
        "status (RunStatus) – Status of the run at a moment in time\n",
        "\n",
        "created_at (datetime) – Date and time when the run was created\n",
        "\n",
        "updated_at (datetime) – Date and time when the run was last updated\n",
        "\n",
        "created_by (str) – Email of the user who created the run\n",
        "\n",
        "priority (str) – Priority of the run\n",
        "\n",
        "preemptible (bool) – Whether the run can be stopped and re-queued by higher priority jobs\n",
        "\n",
        "retry_on_system_failure (bool) – Whether the run should be retried on system failure\n",
        "\n",
        "cluster (str) – Cluster the run is running on\n",
        "\n",
        "gpus (int) – Number of GPUs the run is using\n",
        "\n",
        "gpu_type (str) – Type of GPU the run is using\n",
        "\n",
        "cpus (int) – Number of CPUs the run is using\n",
        "\n",
        "node_count (int) – Number of nodes the run is using\n",
        "\n",
        "latest_resumption (Resumption) – Latest resumption of the run\n",
        "\n",
        "max_retries (Optional[int]) – Maximum number of times the run can be retried\n",
        "\n",
        "reason (Optional[str]) – Reason the run was stopped\n",
        "\n",
        "nodes (List[:class:`~mcli.api.model.run.Node]`) – Nodes the run is using\n",
        "\n",
        "submitted_config (Optional[:class:`~mcli.models.run_config.RunConfig]`) – Submitted run configuration\n",
        "\n",
        "metadata (Optional[Dict[str, Any]]) – Metadata associated with the run\n",
        "\n",
        "last_resumption_id (Optional[str]) – ID of the last resumption of the run\n",
        "\n",
        "resumptions (List[:class:`~mcli.api.model.run.Resumption]`) – Resumptions of the run\n",
        "\n",
        "lifecycle (List[:class:`~mcli.api.model.run.RunLifecycle]`) – Lifecycle of the run\n",
        "\n",
        "image (Optional[str]) – Image the run is using\n",
        "\n",
        "clone(name=None, image=None, cluster=None, instance=None, nodes=None, gpu_type=None, gpus=None, priority=None, preemptible=None, max_retries=None)[source]\n",
        "Submits a new run with the same configuration as this run\n",
        "\n",
        "PARAMETERS\n",
        "name (str) – Override the name of the run\n",
        "\n",
        "image (str) – Override the image of the run\n",
        "\n",
        "cluster (str) – Override the cluster of the run\n",
        "\n",
        "instance (str) – Override the instance of the run\n",
        "\n",
        "nodes (int) – Override the number of nodes of the run\n",
        "\n",
        "gpu_type (str) – Override the GPU type of the run\n",
        "\n",
        "gpus (int) – Override the number of GPUs of the run\n",
        "\n",
        "priority (str) – Override the priority of the run\n",
        "\n",
        "preemptible (bool) – Override whether the run can be stopped and re-queued by higher priority jobs\n",
        "\n",
        "max_retries (int) – Override the max number of times the run can be retried\n",
        "\n",
        "RETURNS\n",
        "New :class:`~mcli.api.model.run.Run` object\n",
        "\n",
        "property completed_at\n",
        "The time the run was completed\n",
        "\n",
        "If there are multiple resumptions, this will be the last end time Completed At will be None if the last resumption has not been completed\n",
        "\n",
        "RETURNS\n",
        "The time the run was last completed\n",
        "\n",
        "property cumulative_pending_time\n",
        "Cumulative time spent in the PENDING state\n",
        "\n",
        "RETURNS\n",
        "The cumulative time (seconds)\n",
        "\n",
        "property cumulative_running_time\n",
        "Cumulative time spent in the RUNNING state\n",
        "\n",
        "RETURNS\n",
        "The cumulative time (seconds)\n",
        "\n",
        "delete()[source]\n",
        "Deletes the run\n",
        "\n",
        "RETURNS\n",
        "Deleted :class:`~mcli.api.model.run.Run` object\n",
        "\n",
        "property display_name\n",
        "The name of the run to display in the CLI\n",
        "\n",
        "RETURNS\n",
        "The name of the run\n",
        "\n",
        "refresh()[source]\n",
        "Refreshes the data on the run object\n",
        "\n",
        "RETURNS\n",
        "Refreshed :class:`~mcli.api.model.run.Run` object\n",
        "\n",
        "property resumption_count\n",
        "Number of times the run has been resumed\n",
        "\n",
        "RETURNS\n",
        "The number of times the run has been resumed\n",
        "\n",
        "property started_at\n",
        "The time the run was first started\n",
        "\n",
        "If there are multiple resumptions, this will be the earliest start time Started At will be None if the first resumption has not been started\n",
        "\n",
        "RETURNS\n",
        "The time the run was first started\n",
        "\n",
        "stop()[source]\n",
        "Stops the run\n",
        "\n",
        "RETURNS\n",
        "Stopped :class:`~mcli.api.model.run.Run` object\n",
        "\n",
        "update(preemptible=None, priority=None, max_retries=None, retry_on_system_failure=None)[source]\n",
        "Updates the run’s data\n",
        "\n",
        "PARAMETERS\n",
        "preemptible (bool) – Update whether the run can be stopped and re-queued by higher priority jobs; default is False\n",
        "\n",
        "priority (str) – Update the priority of the run to low, medium, or high; default is medium\n",
        "\n",
        "max_retries (int) – Update the max number of times the run can be retried; default is 0\n",
        "\n",
        "retry_on_system_failure (bool) – Update whether the run should be retried on system failure (i.e. a node failure); default is False\n",
        "\n",
        "RETURNS\n",
        "Updated :class:`~mcli.api.model.run.Run` object\n",
        "\n",
        "update_metadata(metadata)[source]\n",
        "Updates the run’s metadata\n",
        "\n",
        "PARAMETERS\n",
        "metadata (Dict[str, Any]) – The metadata to update the run with. This will be merged with the existing metadata. Keys not specified in this dictionary will not be modified.\n",
        "\n",
        "RETURNS\n",
        "Updated :class:`~mcli.api.model.run.Run` object\n",
        "\n",
        "Observing a run\n",
        "Getting a run’s logs\n",
        "There are two functions for fetching run logs:\n",
        "\n",
        "get_run_logs(): Gets currently available logs for any run. Ideal for completed runs or checking progress of an active run\n",
        "\n",
        "follow_run_logs(): Follows logs line-by-line for any run. Ideal for monitoring active runs in real time or a condition is reached (see also wait_for_run_status())\n",
        "\n",
        "mcli.api.runs.get_run_logs(run, rank=None, *, timeout=None, future=False, failed=False, resumption=None)[source]\n",
        "Get the current logs for an active or completed run\n",
        "\n",
        "Get the current logs for an active or completed run in the MosaicML platform. This returns the full logs as a str, as they exist at the time the request is made. If you want to follow the logs for an active run line-by-line, use follow_run_logs().\n",
        "\n",
        "PARAMETERS\n",
        "run (str | Run) – The run to get logs for. If a name is provided, the remaining required run details will be queried with get_runs().\n",
        "\n",
        "rank (Optional[int]) – Node rank of a run to get logs for. Defaults to the lowest available rank. This will usually be rank 0 unless something has gone wrong.\n",
        "\n",
        "timeout (Optional[float]) – Time, in seconds, in which the call should complete. If the the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "future (bool) – Return the output as a Future . If True, the call to get_run_logs() will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the log text, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "failed (bool) – Return the logs of the first failed rank for the provided resumption if True. False by default.\n",
        "\n",
        "resumption (Optional[int]) – Resumption (0-indexed) of a run to get logs for. Defaults to the last resumption\n",
        "\n",
        "RETURNS\n",
        "If future is False – The full log text for a run at the time of the request as a str\n",
        "\n",
        "Otherwise – A Future for the log text\n",
        "\n",
        "mcli.api.runs.follow_run_logs(run, rank=None, *, timeout=None, future=False, resumption=None)[source]\n",
        "Follow the logs for an active or completed run in the MosaicML platform\n",
        "\n",
        "This returns a generator of individual log lines, line-by-line, and will wait until new lines are produced if the run is still active.\n",
        "\n",
        "PARAMETERS\n",
        "run (str | Run) – The run to get logs for. If a name is provided, the remaining required run details will be queried with get_runs().\n",
        "\n",
        "rank (Optional[int]) – Node rank of a run to get logs for. Defaults to the lowest available rank. This will usually be rank 0 unless something has gone wrong.\n",
        "\n",
        "timeout (Optional[float]) – Time, in seconds, in which the call should complete. If the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored. A run may take some time to generate logs, so you likely do not want to set a timeout.\n",
        "\n",
        "future (bool) – Return the output as a Future . If True, the call to follow_run_logs() will return immediately and the request will be processed in the background. The generator returned by the ~concurrent.futures.Future will yield a ~concurrent.futures.Future for each new log string returned from the cloud. This takes precedence over the timeout argument. To get the generator, use return_value.result() with an optional timeout argument and log_future.result() for each new log string.\n",
        "\n",
        "RETURNS\n",
        "If future is False – A line-by-line Generator of the logs for a run\n",
        "\n",
        "Otherwise – A Future of a line-by-line generator of the logs for a run\n",
        "\n",
        "Monitoring a run throughout its lifecycle\n",
        "mcli.api.runs.wait_for_run_status(run, status, timeout=None, future=False)[source]\n",
        "Wait for a launched run to reach a specific status\n",
        "\n",
        "PARAMETERS\n",
        "run (str | Run) – The run whose status should be watched. This can be provided using the run’s name or an existing Run object.\n",
        "\n",
        "status (str | RunStatus) – Status to wait for. This can be any valid RunStatus value. If the status is short-lived, or the run terminates, it is possible the run will reach a LATER status than the one requested. If the run never reaches this state (e.g. it stops early or the wait times out), then an error will be raised. See exception details below.\n",
        "\n",
        "timeout (Optional[float]) – Time, in seconds, in which the call should complete. If the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "future (bool) – Return the output as a Future. If True, the call to wait_for_run_status() will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the Run output, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "RAISES\n",
        "MAPIException – Raised if the run does not exist or there is an issue connecting to the MAPI service.\n",
        "\n",
        "RunStatusNotReached – Raised in the event that the watch closes before the run reaches the desired status. If this happens, the connection to MAPI may have dropped, so try again.\n",
        "\n",
        "TimeoutError – Raised if the run did not reach the correct status in the specified time\n",
        "\n",
        "RETURNS\n",
        "If future is False – A Run object once it has reached the requested status\n",
        "\n",
        "Otherwise:\n",
        "A Future for the run. This will not resolve until the run reaches the requested status\n",
        "\n",
        "The RunStatus object\n",
        "The RunStatus object is attached to each Run object and reflects the most recent status the run has been observed with.\n",
        "\n",
        "class mcli.api.runs.RunStatus(value)[source]\n",
        "Possible statuses of a run\n",
        "\n",
        "PENDING = 'PENDING'\n",
        "The run has been submitted and is waiting to be scheduled\n",
        "\n",
        "QUEUED = 'QUEUED'\n",
        "The run is awaiting execution\n",
        "\n",
        "STARTING = 'STARTING'\n",
        "The run is starting up and preparing to run\n",
        "\n",
        "RUNNING = 'RUNNING'\n",
        "The run is actively running\n",
        "\n",
        "TERMINATING = 'TERMINATING'\n",
        "The run is in the process of being terminated\n",
        "\n",
        "COMPLETED = 'COMPLETED'\n",
        "The run has finished without any errors\n",
        "\n",
        "STOPPED = 'STOPPED'\n",
        "The run has stopped\n",
        "\n",
        "FAILED = 'FAILED'\n",
        "The run has failed due to an issue at runtime\n",
        "\n",
        "UNKNOWN = 'UNKNOWN'\n",
        "A valid run status cannot be found\n",
        "\n",
        "before(other, inclusive=False)[source]\n",
        "Returns True if this state usually comes “before” the other\n",
        "\n",
        "PARAMETERS\n",
        "other – Another RunStatus\n",
        "\n",
        "inclusive – If True, equality evaluates to True. Default False.\n",
        "\n",
        "RETURNS\n",
        "If this state is “before” the other\n",
        "\n",
        "EXAMPLE\n",
        "\n",
        "RunStatus.RUNNING.before(RunStatus.COMPLETED)\n",
        "True\n",
        "RunStatus.PENDING.before(RunStatus.RUNNING)\n",
        "True\n",
        "after(other, inclusive=False)[source]\n",
        "Returns True if this state usually comes “after” the other\n",
        "\n",
        "PARAMETERS\n",
        "other – Another RunStatus\n",
        "\n",
        "inclusive – If True, equality evaluates to True. Default False.\n",
        "\n",
        "RETURNS\n",
        "If this state is “after” the other\n",
        "\n",
        "EXAMPLE\n",
        "\n",
        "RunStatus.COMPLETED.after(RunStatus.RUNNING)\n",
        "True\n",
        "RunStatus.RUNNING.after(RunStatus.PENDING)\n",
        "True\n",
        "is_terminal()[source]\n",
        "Returns True if this state is terminal\n",
        "\n",
        "RETURNS\n",
        "If this state is terminal\n",
        "\n",
        "EXAMPLE\n",
        "\n",
        "RunStatus.RUNNING.is_terminal()\n",
        "False\n",
        "RunStatus.COMPLETED.is_terminal()\n",
        "True\n",
        "classmethod from_string(run_status)[source]\n",
        "Convert a string to a valid RunStatus Enum\n",
        "\n",
        "If the run status string is not recognized, will return RunStatus.UNKNOWN instead of raising a KeyError\n",
        "\n",
        "Listing runs\n",
        "All runs that you have launched in the MosaicML platform and have not deleted can be accessed using the get_runs() function. Optional filters allow you to specify a subset of runs to list by name, cluster, gpu type, gpu number, or status.\n",
        "\n",
        "mcli.api.runs.get_runs(runs=None, *, cluster_names=None, before=None, after=None, gpu_types=None, gpu_nums=None, statuses=None, timeout=10, future=False, clusters=None, user_emails=None, include_details=False, limit=None, include_interactive=None)[source]\n",
        "List runs that have been launched in the MosaicML platform\n",
        "\n",
        "The returned list will contain all of the details stored about the requested runs.\n",
        "\n",
        "PARAMETERS\n",
        "runs – List of runs on which to get information\n",
        "\n",
        "cluster_names – List of cluster names to filter runs. This can be a list of str or :type Cluster: objects. Only runs submitted to these clusters will be returned.\n",
        "\n",
        "before – Only runs created strictly before this time will be returned. This can be a str in ISO 8601 format(e.g 2023-03-31T12:23:04.34+05:30) or a datetime object.\n",
        "\n",
        "after – Only runs created at or after this time will be returned. This can be a str in ISO 8601 format(e.g 2023-03-31T12:23:04.34+05:30) or a datetime object.\n",
        "\n",
        "gpu_types – List of gpu types to filter runs. This can be a list of str or :type GPUType: enums. Only runs scheduled on these GPUs will be returned.\n",
        "\n",
        "gpu_nums – List of gpu counts to filter runs. Only runs scheduled on this number of GPUs will be returned.\n",
        "\n",
        "statuses – List of run statuses to filter runs. This can be a list of str or :type RunStatus: enums. Only runs currently in these phases will be returned.\n",
        "\n",
        "timeout – Time, in seconds, in which the call should complete. If the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "future – Return the output as a :type concurrent.futures.Future:. If True, the call to get_runs will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the list of runs, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "include_details – If true, will fetch detailed information like run input for each run.\n",
        "\n",
        "limit – Maximum number of runs to return. If None, all runs will be returned.\n",
        "\n",
        "include_interactive – Whether the run is interactive or not. If None, all runs will be returned.\n",
        "\n",
        "RAISES\n",
        "MAPIException – If connecting to MAPI, raised when a MAPI communication error occurs\n",
        "\n",
        "Updating runs\n",
        "mcli.api.runs.update_run(run, update_run_data=None, *, preemptible=None, priority=None, max_retries=None, retry_on_system_failure=None, timeout=10, future=False, max_duration=None)[source]\n",
        "Update a run’s data in the MosaicML platform.\n",
        "\n",
        "Any values that are not specified will not be modified.\n",
        "\n",
        "PARAMETERS\n",
        "run (Optional[str | ``:class:`~mcli.api.model.run.Run` ``]) – A run or run name to update. Using Run objects is most efficient. See the note below.\n",
        "\n",
        "update_run_data (Dict[str, Any]) – DEPRECATED: Use the individual named-arguments instead. The data to update the run with. This can include preemptible, priority, maxRetries, and retryOnSystemFailure\n",
        "\n",
        "preemptible (bool) – Update whether the run can be stopped and re-queued by higher priority jobs; default is False\n",
        "\n",
        "priority (str) – Update the priority of the run to low, medium, or high; default is medium\n",
        "\n",
        "max_retries (int) – Update the max number of times the run can be retried; default is 0\n",
        "\n",
        "retry_on_system_failure (bool) – Update whether the run should be retried on system failure (i.e. a node failure); default is False\n",
        "\n",
        "timeout (Optional[float]) – Time, in seconds, in which the call should complete. If the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "max_duration – Update the max time that a run can run for (in hours).\n",
        "\n",
        "future (bool) – Return the output as a Future. If True, the call to update_run() will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the list of Run output, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "RAISES\n",
        "MAPIException – Raised if updating the requested run failed\n",
        "\n",
        "RETURNS\n",
        "If future is False – Updated Run object\n",
        "\n",
        "Otherwise – A Future for the list\n",
        "\n",
        "Stopping runs\n",
        "mcli.api.runs.stop_run(run, *, timeout=10, future=False)[source]\n",
        "Stop a run\n",
        "\n",
        "Stop a run currently running in the MosaicML platform.\n",
        "\n",
        "PARAMETERS\n",
        "run (Optional[str | ``:class:`~mcli.api.model.run.Run` ``]) – A run or run name to stop. Using Run objects is most efficient. See the note below.\n",
        "\n",
        "timeout (Optional[float]) – Time, in seconds, in which the call should complete. If the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "future (bool) – Return the output as a Future. If True, the call to stop_run() will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the list of Run output, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "RAISES\n",
        "MAPIException – Raised if stopping the requested runs failed A successfully stopped run will have the status `RunStatus.STOPPED`\n",
        "\n",
        "RETURNS\n",
        "If future is False – Stopped Run object\n",
        "\n",
        "Otherwise – A Future for the object\n",
        "\n",
        "mcli.api.runs.stop_runs(runs, *, timeout=10, future=False)[source]\n",
        "Stop a list of runs\n",
        "\n",
        "Stop a list of runs currently running in the MosaicML platform.\n",
        "\n",
        "PARAMETERS\n",
        "runs (Optional[List[str] | List[Run ]]) – A list of runs or run names to stop. Using Run objects is most efficient. See the note below.\n",
        "\n",
        "timeout (Optional[float]) – Time, in seconds, in which the call should complete. If the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "future (bool) – Return the output as a Future. If True, the call to stop_runs() will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the list of Run output, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "RAISES\n",
        "MAPIException – Raised if stopping any of the requested runs failed. All successfully stopped runs will have the status `RunStatus.STOPPED`. You can freely retry any stopped and unstopped runs if this error is raised due to a connection issue.\n",
        "\n",
        "RETURNS\n",
        "If future is False – A list of stopped Run objects\n",
        "\n",
        "Otherwise – A Future for the list\n",
        "\n",
        "Deleting runs\n",
        "To delete runs, you must supply the run names or Run object. To delete a set of runs, you can use the output of get_runs() or even define your own filters directly:\n",
        "\n",
        "# delete a run by name\n",
        "delete_run('delete-this-run')\n",
        "\n",
        "# delete failed runs on cluster xyz using 1 or 2 GPUs\n",
        "failed_runs = get_runs(statuses=['FAILED'], cluster_names=['xyz'], gpu_nums=[1, 2])\n",
        "delete_runs(failed_runs)\n",
        "\n",
        "# delete completed runs older than a month with name pattern\n",
        "completed = get_runs(statuses=['COMPLETED'])\n",
        "ref_date = dt.datetime.now() - dt.timedelta(days=30)\n",
        "old_runs = [r for r in completed if 'experiment1' in r.name and r.created_at < ref_date ]\n",
        "delete_runs(old_runs)\n",
        "mcli.api.runs.delete_run(run, *, timeout=10, future=False)[source]\n",
        "Delete a run in the MosaicML platform\n",
        "\n",
        "If a run is currently running, it will first be stopped.\n",
        "\n",
        "PARAMETERS\n",
        "run – A run to delete\n",
        "\n",
        "timeout – Time, in seconds, in which the call should complete. If the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "future – Return the output as a :type concurrent.futures.Future:. If True, the call to delete_run will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the :type Run: output, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "RETURNS\n",
        "A – type Run: for the run that was deleted\n",
        "\n",
        "mcli.api.runs.delete_runs(runs, *, timeout=10, future=False)[source]\n",
        "Delete a list of runs in the MosaicML platform\n",
        "\n",
        "Any runs that are currently running will first be stopped.\n",
        "\n",
        "PARAMETERS\n",
        "runs – A list of runs or run names to delete\n",
        "\n",
        "timeout – Time, in seconds, in which the call should complete. If the call takes too long, a TimeoutError will be raised. If future is True, this value will be ignored.\n",
        "\n",
        "future – Return the output as a :type concurrent.futures.Future:. If True, the call to delete_run will return immediately and the request will be processed in the background. This takes precedence over the timeout argument. To get the :type Run: output, use return_value.result() with an optional timeout argument.\n",
        "\n",
        "RETURNS\n",
        "A list of – type Run: for the runs that were deleted\"\"\",\n",
        "    \"\"\"Interactive Runs\n",
        "Interactive runs give the ability to debug and iterate quickly inside your cluster in a secure way. Interactivity works on top of the existing MosaicML runs, so before connecting a run workload needs to be submitted to the cluster. For security purposes storage is not persisted, so we recommend utilizing your own cloud storage and git repositories to stream and save data between runs.\n",
        "\n",
        "Launch an interactive run\n",
        "Launching new runs\n",
        "\n",
        "All runs on reserved clusters can be connected to, regardless of how they were launched. This section goes over mcli interactive, which is a helpful alias for creating simple “sleeper” runs for interactive purposes. You can also create a custom run configuration for interactive purposes through the normal mcli run entrypoint\n",
        "\n",
        "Launch an interactive run by running:\n",
        "\n",
        "mcli interactive --max-duration 1 --gpus 1 --tmux --cluster <cluster-name>\n",
        "This command creates a “sleeper” run that will last for 1 hour (--max-duration 1), request 1 GPU (--gpus 1) and connect to a tmux session (--tmux) within your run. The --max-duration or --hours argument is required to avoid any large, accidental charges from a forgotten run. The --tmux argument is strongly recommended to allow your session to persist through any temporary disconnects. mcli will automatically try to reconnect you to your run whenever you disconnect, so utilizing tmux dramatically improves this experience.\n",
        "\n",
        "Note that interactive runs act like normal runs:\n",
        "\n",
        "\n",
        "BASH\n",
        "# see interactive runs on the cluster\n",
        "mcli util <cluster-name>\n",
        "\n",
        "# your interactive runs will show up when you call \"get runs\"\n",
        "mcli get runs --cluster <cluster-name>\n",
        "\n",
        "# get more info about your run\n",
        "mcli describe run <interactive-run-name>\n",
        "\n",
        "# stop your interactive run early\n",
        "mcli stop run <interactive-run-name>\n",
        "\n",
        "# delete it\n",
        "mcli delete run <interactive-run-name>\n",
        "\n",
        "PYTHON\n",
        "Full documentation for the interactive command\n",
        "Update a run’s max duration\n",
        "After creating an interactive run, you can change its maximum duration.\n",
        "\n",
        "mcli update run <interactive-run-name> --max-duration <hours>\n",
        "Connect to a run in the terminal\n",
        "Regardless of how you launched the run, you can connect to any running run using:\n",
        "\n",
        "mcli connect <run-name> --tmux\n",
        "By default, the session will connect inside a bash shell. We highly recommend using tmux as the entrypoint for your run so your session is robust to disconnects (such as a local internet outage). You can also configure a command other than bash or tmux to execute in the run:\n",
        "\n",
        "mcli connect --command \"top\"\n",
        "If you are running multi-node interactive runs, you can specify the zero-indexed node rank via:\n",
        "\n",
        "mcli connect --rank 2\n",
        "Connect to a run with VSCode\n",
        "Disclaimer\n",
        "\n",
        "Due to VSCode Server licensing, we cannot integrate directly with the native VS code remote development extensions. This guide outlines and documents how to get started with the VSCode server using tunneling\n",
        "\n",
        "First time local setup: Install VSCode and the remote development extension pack. We recommend reviewing the system requirements and installation guide for the extension pack as some requirements are highly dependent on your operating system.\n",
        "\n",
        "Step 1: Create an interactive run as documented above\n",
        "\n",
        "Step 2: Connect to that run via mcli connect\n",
        "\n",
        "Step 3: Run the following commands to download VS Code server and start it:\n",
        "\n",
        "trap '/tmp/code tunnel unregister' EXIT\n",
        "cd /tmp && curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&os=cli-alpine-x64' --output vscode_cli.tar.gz\n",
        "tar -xf vscode_cli.tar.gz\n",
        "/tmp/code tunnel --accept-server-license-terms --no-sleep --name mml-dev-01\n",
        "This will output something like:\n",
        "\n",
        "*\n",
        "* Visual Studio Code Server\n",
        "*\n",
        "* By using the software, you agree to\n",
        "* the Visual Studio Code Server License Terms (https://aka.ms/vscode-server-license) and\n",
        "* the Microsoft Privacy Statement (https://privacy.microsoft.com/en-US/privacystatement).\n",
        "*\n",
        "To grant access to the server, please log into https://github.com/login/device and use code ABCD-1234\n",
        "Step 4: Authenticate using the code provided at https://github.com/login/device and authorize your github account\n",
        "\n",
        "Authenticate using the code provided\n",
        "\n",
        "Authorize your github account\n",
        "\n",
        "Step 5: From an existing VSCode window, connect using remote tunnel by selecting the blue remote window button on the very left of bottom sidebar. Select “Connect to tunnel” from “Remote-Tunnels” and then select the tunnel name (default: “mml-dev-01”)\n",
        "\n",
        "\n",
        "\n",
        "Alternatively, you can connect in the browser using: https://vscode.dev/tunnel/mml-dev-01/tmp\"\"\",\n",
        "    \"\"\"Common Commands\n",
        "mcli deploy -f <your_yaml>\n",
        "Submits an inference deployment with the provided YAML configuration.\n",
        "\n",
        "mcli get deployments\n",
        "Lists all of your inference deployments (see mcli get deployments --help to view the many filters available)\n",
        "\n",
        "mcli describe deployment <deployment_name>\n",
        "Shows detailed information about an inference deployment, including the config that was used to launch it.\n",
        "\n",
        "mcli get deployment logs <deployment_name>\n",
        "Retrieves the console logs of the inference deployment.\n",
        "\n",
        "mcli delete deployment <deployment_name>\n",
        "Deletes the inference deployment from the cluster.\n",
        "\n",
        "mcli update deployment <deployment_name> --image <image>?\n",
        "Updates the image of a deployment.\"\"\",\n",
        "    \"\"\"Configure a deployment\n",
        "Deployment submissions to the MosaicML platform can be configured through a YAML file or using our Python API’s InferenceDeploymentConfig class.\n",
        "\n",
        "The fields are identical across both methods:\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "name\n",
        "\n",
        "required\n",
        "\n",
        "str\n",
        "\n",
        "compute\n",
        "\n",
        "required\n",
        "\n",
        "ComputeConfig\n",
        "\n",
        "replicas\n",
        "\n",
        "optional (default 1)\n",
        "\n",
        "int\n",
        "\n",
        "image\n",
        "\n",
        "optional (default mosaicml/inference)\n",
        "\n",
        "str\n",
        "\n",
        "command\n",
        "\n",
        "optional (default '')\n",
        "\n",
        "str\n",
        "\n",
        "model\n",
        "\n",
        "optional (default None)\n",
        "\n",
        "ModelConfig\n",
        "\n",
        "batching\n",
        "\n",
        "optional (default {max_batch_size: 1, max_timeout_ms: 1000})\n",
        "\n",
        "BatchingConfig\n",
        "\n",
        "integrations\n",
        "\n",
        "optional (default [])\n",
        "\n",
        "List[Dict]\n",
        "\n",
        "env_variables\n",
        "\n",
        "optional (default [])\n",
        "\n",
        "List[Dict]\n",
        "\n",
        "metadata\n",
        "\n",
        "optional (default {})\n",
        "\n",
        "Dict[str, Any]\n",
        "\n",
        "Here’s an example deployment configuration:\n",
        "\n",
        "\n",
        "YAML\n",
        "name: deployment-name\n",
        "compute:\n",
        "  cluster: <my-cluster>\n",
        "  gpu_type: <my-gpu_type>\n",
        "  gpus: 1\n",
        "integrations:\n",
        "  - integration_type: git_repo\n",
        "    git_repo: mosaicml/examples\n",
        "    ssh_clone: false\n",
        "model:\n",
        "  download_parameters:\n",
        "    s3_path: s3://my-checkpoint-path\n",
        "  model_parameters:\n",
        "    task: text-generation\n",
        "    model_dtype: fp16\n",
        "    autocast_dtype: bf16\n",
        "    model_name_or_path: my/local/s3_path\n",
        "metadata:\n",
        "  model_version: 2\n",
        "\n",
        "PYTHON\n",
        "Field Types\n",
        "Deployment Name\n",
        "A deployment name is the primary identifier for working with deployments. For each deployment, a unique identifier is automatically appended to the provided deployment name. After submitting a deployment, the finalized unique deployment is displayed in the terminal, and can also be viewed with mcli get deployments or InferenceDeployment object.\n",
        "\n",
        "Compute Fields\n",
        "The compute field specifies which compute resources to request for a single replica of your inference deployment. See the replicas section for details on how replicas interfaces with compute.\n",
        "\n",
        "In cases where you underspecify compute, the MosaicML platform will try and infer which compute resources to use automatically. Which fields are required depend on which and what type of clusters are available to your organization. If those resources are not valid or if there are multiple options still available, an error will be raised on run submissions, and the run will not be created.\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "Details\n",
        "\n",
        "cluster\n",
        "\n",
        "str\n",
        "\n",
        "Required\n",
        "\n",
        "gpus\n",
        "\n",
        "int\n",
        "\n",
        "Typically required, unless you specify instance or a cpu-only run\n",
        "\n",
        "gpu_type\n",
        "\n",
        "str\n",
        "\n",
        "Optional. Not needed if you specify instance.\n",
        "\n",
        "instance\n",
        "\n",
        "str\n",
        "\n",
        "Optional. Use if the cluster has multiple instances with the same GPU type (ex. 1-wide and 2-wide A10 instances)\n",
        "\n",
        "cpus\n",
        "\n",
        "int\n",
        "\n",
        "Optional. Typically not used other than for debugging small deployments.\n",
        "\n",
        "You can see clusters, instances, and compute resources available to you using:\n",
        "\n",
        "mcli get clusters\n",
        "For example, you can launch a multi-node cluster my-cluster with 16 A100 GPUs:\n",
        "\n",
        "compute:\n",
        "  cluster: my-cluster\n",
        "  gpus: 16\n",
        "  gpu_type: a100_80gb\n",
        "You can also specify a cluster and instance name within that cluster as follows:\n",
        "\n",
        "compute:\n",
        "  cluster: my-cluster\n",
        "  instance: oci.vm.gpu.a10.2\n",
        "In the above case, the deployment will use all the GPUs on the instance by default. If you want to use fewer GPUs, you can also specify the gpus field using a value up to the total number of GPUs available on the instance.\n",
        "\n",
        "Replicas\n",
        "If the value of replicas is n > 1 in your deployment YAML, then the deployment will spawn n copies of whatever you request in the compute field.\n",
        "\n",
        "For example, if your YAML looks like this:\n",
        "\n",
        "compute:\n",
        "  cluster: my-cluster\n",
        "  gpus: 1\n",
        "  gpu_type: a100_40gb\n",
        "replicas: 2\n",
        "then your deployment will spawn 2 replicas each using 1 GPU. Since you did not specify an instance in the compute field, each replica will run on any instance that has a matching GPU type and 1 free GPU.\n",
        "\n",
        "As another example, if your deployment YAML looks like this:\n",
        "\n",
        "compute:\n",
        "  cluster: my-cluster\n",
        "  instance: oci.vm.gpu.a10.2\n",
        "replicas: 2\n",
        "then your deployment will spawn 2 replicas with each one being on a oci.vm.gpu.a10.2 instance. Since that particular instance has 2 GPUs, your deployment will use 4 GPUs total (2 replicas X 2 GPUs per replica).\n",
        "\n",
        "Model\n",
        "The provided model parameters are mounted as a YAML file of your deployment at /mnt/model/model_config.yaml for your code to access. These parameters configure the out-of-the-box MosaicML inference server. If you choose not to provide a model config, we submit the deployment under the assumption that you’re specifying your own inference server code in the provided image under port 8080.\n",
        "\n",
        "The model schema fields are as follows:\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "Details\n",
        "\n",
        "downloader\n",
        "\n",
        "str\n",
        "\n",
        "The module path to the function that downloads any necessary model files (i.e. checkpoint files). If not provided, uses the default downloader described below.\n",
        "\n",
        "download_parameters\n",
        "\n",
        "Dict[str, Any]\n",
        "\n",
        "Kwargs passed into the downloader function\n",
        "\n",
        "model_handler\n",
        "\n",
        "str\n",
        "\n",
        "The module path to the model handler class. If not provided, defaults to the HuggingFace model handler that comes with the inference server.\n",
        "\n",
        "model_parameters\n",
        "\n",
        "Dict[str, Any]\n",
        "\n",
        "Kwargs used to initialize your model handler\n",
        "\n",
        "Default Download Parameters\n",
        "\n",
        "If you’re using the default downloader that comes with the inference server, the parameters are as follows:\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "Details\n",
        "\n",
        "hf_path\n",
        "\n",
        "str\n",
        "\n",
        "The name of the HuggingFace model repo\n",
        "\n",
        "s3_path\n",
        "\n",
        "str\n",
        "\n",
        "The s3 path to a model checkpoint in the HuggingFace format\n",
        "\n",
        "gcp_path\n",
        "\n",
        "str\n",
        "\n",
        "The gcp path to a model checkpoint in the HuggingFace format\n",
        "\n",
        "You can only specify one of the above options.\n",
        "\n",
        "Default HF Model Parameters\n",
        "\n",
        "If you’re using the default HuggingFace model handler that comes with the inference server, the parameters are as follows:\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "Details\n",
        "\n",
        "task\n",
        "\n",
        "str\n",
        "\n",
        "Required. Determines how the forward pass is computed. Currently only text-generation and feature-extraction are supported, with more to come!\n",
        "\n",
        "model_dtype\n",
        "\n",
        "str\n",
        "\n",
        "The dtype that a Hugging Face model gets loaded as. Defaults to bf16\n",
        "\n",
        "autocast_dtype\n",
        "\n",
        "str\n",
        "\n",
        "The dtype that the model gets autocasted to if provided. Defaults to None\n",
        "\n",
        "model_name_or_path\n",
        "\n",
        "str\n",
        "\n",
        "The name of the HuggingFace repo of the model to load or the path of the locally downloaded HuggingFace model\n",
        "\n",
        "Please note that the out-of-the-box MosaicML webserver does not support multi-gpu inference for the following Hugging Face model families: CodeGen, DeBERTa, FlauBERT, FSMT, GPT-2, LED, Longformer, XLM, XLNet.\n",
        "\n",
        "Custom Model Handler Format\n",
        "\n",
        "See the docs on custom model handlers for details how to implement your own model handler class.\n",
        "\n",
        "Batching\n",
        "The configuration for dynamic batching in the web server.\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "Details\n",
        "\n",
        "max_batch_size\n",
        "\n",
        "int\n",
        "\n",
        "The maximum batch size to create before sending requests to the model.\n",
        "\n",
        "max_timeout_ms\n",
        "\n",
        "int\n",
        "\n",
        "The maximum time to wait from the first request before sending requests to the model.\n",
        "\n",
        "Setting max_batch_size to 1 is equivalent to turning dynamic batching off which is the default behavior if batching is not specified.\n",
        "\n",
        "Image\n",
        "Deployments are executed within Docker containers defined by a Docker image. Images on DockerHub can be configured as <organization>/<image name>. For private Dockerhub repositories, add a docker secret with:\n",
        "\n",
        "mcli create secret docker\n",
        "For more details, see the Docker Secret Page.\n",
        "\n",
        "Using Alternative Docker Registries\n",
        "\n",
        "While we default to DockerHub, custom registries are supported, see Docker’s documentation and Docker Secret Page for more details.\n",
        "\n",
        "Command\n",
        "The command is what’s executed when the deployments starts, typically to start the inference server. For example, the following command:\n",
        "\n",
        "command: |\n",
        "  echo Hello World!\n",
        "will result in a deployment that prints “Hello World” to the console.\n",
        "\n",
        "If you are using a support model format (Hugging Face, Custom Model) then the command field is optional and will be populated by default as the launch command for starting the MosaicML inference server.\n",
        "\n",
        "Integrations\n",
        "We support many Integrations to customize aspects of both the deployment setup and environment.\n",
        "\n",
        "Integrations are specified as a list in the YAML. Each item in the list must specify a valid integration_type along with the relevant fields for the requested integration.\n",
        "\n",
        "Some examples of integrations include automatically cloning a Github repository, installing python packages as shown below:\n",
        "\n",
        "integrations:\n",
        "  - integration_type: git_repo\n",
        "    git_repo: org/my_repo\n",
        "    git_branch: my-work-branch\n",
        "You can read more about integrations on the Integrations Page.\n",
        "\n",
        "Some integrations may require adding secrets. For example, pulling from a private github repository would require the git-ssh secret to be configured. See the Secrets Page.\n",
        "\n",
        "Environment Variables\n",
        "Environment variables can also be injected into each deployment at runtime through the env_variables field. Each environment variable in the list must have a key and value configured.\n",
        "\n",
        "key: name used to access the value of the environment variable\n",
        "\n",
        "value: value of the environment variable.\n",
        "\n",
        "For example, the below YAML will print “Hello MOSAICML my name is MOSAICML_TWO!”:\n",
        "\n",
        "name: hello-world\n",
        "image: python\n",
        "command: |\n",
        "  sleep 2\n",
        "  echo Hello $NAME my name is $SECOND_NAME!\n",
        "env_variables:\n",
        "  - key: NAME\n",
        "    value: MOSAICML\n",
        "  - key: SECOND_NAME\n",
        "    value: MOSAICML_TWO\n",
        "The command accesses the value of the environment variable by the key field (in this case $NAME and $SECOND_NAME)\n",
        "\n",
        "Metadata\n",
        "Metadata is meant to be a multi-purposed, unstructured place to put information about a deployment. It can be set at the beginning of the deployment, for example to add custom version tags:\n",
        "\n",
        "name: hello-world\n",
        "image: bash\n",
        "command: echo 'hello world'\n",
        "metadata:\n",
        "  model_version: 2\n",
        "Metadata on your deployment is readable through the CLI or SDK:\n",
        "\n",
        "\n",
        "BASH\n",
        "\n",
        "PYTHON\n",
        "from mcli import get_deployment\n",
        "\n",
        "deployment = get_deployment('hello-world-VC5nFs')\n",
        "print(deployment.metadata)\n",
        "# {\"model_version\": 2}\n",
        "Metadata size constraints\n",
        "\n",
        "Metadata is not intended for large amounts of data such as time series data. Each key is limited to 200 characters and value is limited to 0.1mb. Metadata cannot have more than 200 keys. A MAPIException will be raised on creation or updates if any of these limits are exceeded.\"\"\",\n",
        "    \"\"\"Deployments\n",
        "Below outlines how to work with deployments, including creating, updating, getting, and deleting deployments as well as pinging the deployment, and sending requests to your deployment.\n",
        "\n",
        "Creating a deployment\n",
        "Deployments can programmatically be created, giving you flexibility to define custom workflows or create similar deployments in quick succession. create_inference_deployment() will takes a InferenceDeploymentConfig object, which is a fully-configured deployment ready to launch. The method will launch the inference deployment and then return a InferenceDeployment object, which includes the InferenceDeploymentConfig data in InferenceDeployment.config but also data received at the time the deployment was launched.\n",
        "\n",
        "The InferenceDeploymentConfig object\n",
        "The InferenceDeploymentConfig object holds configuration data needed to launch a deployment. This is the underlying python data structure MCLI uses, so before beginning make sure to familiarize yourself with the inference schema. Take a look at the API Reference for the full list of fields on the InferenceDeploymentConfig object.\n",
        "\n",
        "There are two ways to initialize a InferenceDeploymentConfig object that can be used to configure and create a deployment. The first is by referencing a YAML file, equivalent to the file argument in MCLI:\n",
        "\n",
        "from mcli import InferenceDeploymentConfig, create_inference_deployment\n",
        "\n",
        "deployment_config = InferenceDeploymentConfig.from_file('hello_world.yaml')\n",
        "created_deployment = create_inference_deployment(deployment_config)\n",
        "Alternatively, you can instantiate the InferenceDeploymentConfig object directly in python:\n",
        "\n",
        "from mcli import InferenceDeploymentConfig, create_inference_deployment\n",
        "\n",
        "cluster = \"<your-cluster>\"\n",
        "inference_deployment_config = InferenceDeploymentConfig(\n",
        "    name='hello-world',\n",
        "    image='bash',\n",
        "    command='echo \"Hello World!\" && sleep 60',\n",
        "    gpu_type='none',\n",
        "    cluster=cluster,\n",
        ")\n",
        "create_deployment = create_inference_deployment(inference_deployment_config)\n",
        "These can also be used in combination, for example loading a base configuration file and modifying select fields:\n",
        "\n",
        "from mcli import InferenceDeploymentConfig, create_inference_deployment\n",
        "\n",
        "special_config = InferenceDeploymentConfig.from_file('base_config.yaml')\n",
        "special_config.metadata = {\"version\": 1}\n",
        "created_deployment = create_inference_deployment(special_config)\n",
        "The InferenceDeployment object\n",
        "Created deployments will be returned as an InferenceDeployment object in create_inference_deployment(). This object can be used as input to any subsequent deployment function, for example you can start a deployment and then immediately ping the deployment to see if it’s ready.\n",
        "\n",
        "from mcli import create_inference_deployment, ping_inference_deployment as ping\n",
        "\n",
        "created_deployment = create_inference_deployment(config)\n",
        "ping(created_deployment)\n",
        "Querying a deployment\n",
        "When querying your inference deployment, you must provide a JSON with a key called inputs in the request. This will typicaly be a list of inputs to the model. For example, in a text-to-text language model the inputs field will contain a list of strings to be tokenized and fed into the model.\n",
        "\n",
        "Optionally, you can also provide a parameters field which contains hyperparameters used in the forward pass of your model. An example of where one might use the parameters field is to pass arguments to the generation pipeline in a text-to-text language model. See our docs on this for more details.\n",
        "\n",
        "The reason parameters is separated out from inputs in the request is so that the webserver’s dynamic batching functionality can automatically group requests with the same sets of parameters together in the batches it creates. This is important because in some cases different sets of parameters cannot be grouped together when running inference. For example, consider grouping different max_output_sequence_length parameters together in a text-to-text language model. The result would be that the user’s model handler class would have to implement logic to handle this. Separating out parameters makes it possible for the user to write a handler class without having to consider these details.\n",
        "\n",
        "An example request is shown below:\n",
        "\n",
        "{\n",
        "  \"inputs\": [\"(required) <any JSON value>\"],\n",
        "  \"parameters\": \"(optional) <any JSON value>\"\n",
        "}\n",
        "Observing a deployment\n",
        "Getting a deployment’s logs\n",
        "get_inference_deployment_logs() gets currently available logs for any deployment.\n",
        "\n",
        "from mcli import create_inference_deployment, get_inference_deployment_logs\n",
        "\n",
        "created_deployment = create_inference_deployment(config)\n",
        "logs = get_inference_deployment_logs(created_deployment)\n",
        "Listing deployments\n",
        "All deployments from your organization that have been launched through the MosaicML platform and have not deleted can be accessed using the get_inference_deployments() function. Optional filters allow you to specify a subset of deployments to list by name, cluster, gpu type, gpu number, or status.\n",
        "\n",
        "from mcli import get_inference_deployments\n",
        "\n",
        "listed_deployments = get_inference_deployments(gpu_nums=1)\n",
        "Updating a deployment\n",
        "To update a deployment, you must supply the deployment names or InferenceDeployment object and the fields that need to be updated.\n",
        "\n",
        "To update a set of deployments, you can use the output of get_inference_deployments() or even define your own filters directly:\n",
        "\n",
        "Currently, we support the following fields:\n",
        "\n",
        "image : Takes a string value.\n",
        "\n",
        "replicas : Takes an int value\n",
        "\n",
        "metadata: Takes a dict value of metadata keys (strings) and values (any).\n",
        "\n",
        "from mcli import update_inference_deployment\n",
        "\n",
        "update_inference_deployment('deployment-name', {\"metadata\":'{\"name\":\"my_first_model\"}', \"replicas\":2, \"image\":\"my_new_image\"})\n",
        "\n",
        "from mcli import update_inference_deployments, get_inference_deployments\n",
        "\n",
        "to_update = get_inference_deployments(cluster=\"name\")\n",
        "update_inference_deployments(to_update, {\"replicas\": 3})\n",
        "Deleting deployments\n",
        "To delete deployments, you must supply the deployment names or InferenceDeployment object. To delete a set of deployments, you can use the output of get_inference_deployments() or even define your own filters directly:\n",
        "\n",
        "from mcli import delete_inference_deployment\n",
        "\n",
        "delete_inference_deployment('delete-this-deployment')\n",
        "\n",
        "from mcli import delete_inference_deployments, get_inference_deployments\n",
        "\n",
        "\n",
        "to_delete = get_inference_deployments(cluster=\"name\")\n",
        "delete_inference_deployments(to_delete)\n",
        "Pinging a deployment\n",
        "You can ping a deployment to determine the server status. We return a status code 200 when the server is live, which indicates the model has finished loading and is ready to accept requests. You can either pass in a name or a InferenceDeployment object.\n",
        "\n",
        "from mcli import ping\n",
        "\n",
        "ping('deployment-name')\n",
        "Sending predictions to a deployment\n",
        "You can send predictions to your deployment programmatically. There are 3 ways you can specify the deployment you’d like to send your request to:\n",
        "\n",
        "You can pass in the deployment object returned from create_inference_deployment or get_inference_deployment.\n",
        "\n",
        "from mcli import predict\n",
        "\n",
        "deployment = get_inference_deployments(name='your-deployment-name')\n",
        "predict(deployment, {'inputs': ['some input']})\n",
        "You can pass in the url to the deployment.\n",
        "\n",
        "from mcli import predict\n",
        "\n",
        "predict('https://your-deployment.inf.hosted-on.mosaicml.hosting', {'inputs': ['some input']})\n",
        "You can pass in the name of the deployment.\n",
        "\n",
        "from mcli import predict\n",
        "\n",
        "predict('your-deployment-name', {'inputs': ['some input']})\n",
        "Getting metrics for a deployment\n",
        "You can retrieve latency, throughput, error rate and cpu utilization metrics from the /metrics endpoint on the deployment. These metrics are compute over the past 1 hour at 1 minute interval.\n",
        "\n",
        "curl https://{deployment-name}.inf.hosted-on.mosaicml.hosting/metrics -H \"Authorization: {api-key}\"\n",
        "Sample response here:\n",
        "\n",
        "{\n",
        "  \"status\": 200,\n",
        "  \"metrics\": {\n",
        "    \"error_rate\": [\n",
        "      [\"2023-05-01 16:24:57\", \"10\"],\n",
        "      [\"2023-05-01 16:23:57\", \"10\"],\n",
        "      ...\n",
        "    ],\n",
        "    \"cpu_seconds\": [\n",
        "      [\"2023-05-01 16:24:57\", \"0.006\"],\n",
        "      [\"2023-05-01 16:23:57\", \"0.001\"],\n",
        "      ...\n",
        "    ],\n",
        "    \"avg_latency\": [\n",
        "      [\"2023-05-01 16:24:57\", \"1.2\"],\n",
        "      [\"2023-05-01 16:23:57\", \"1.5\"],\n",
        "      ...\n",
        "    ],\n",
        "    \"requests_per_second\": [\n",
        "      [\"2023-05-01 16:24:57\", \"0.5\"],\n",
        "      [\"2023-05-01 16:23:57\", \"1.2\"],\n",
        "      ...\n",
        "    ]\n",
        "  }\n",
        "}\"\"\",\n",
        "    \"\"\"Custom Deployments\n",
        "There are two ways we allow you to customize your inference deployment. You can provide your own downloader function or model handler implementation. In this section, we’ll cover the interface for each of these and show you how to use them.\n",
        "\n",
        "Model Handlers\n",
        "Model handlers allow you to define how your model should be loaded and what should happen in a forward pass. They allow for the MosaicML platform to support a wide variety of models and use cases. This is configured by the model_handler field in your deployment input yaml, which expects a python path to your model handler class, and the model_parameters field which expects a key-value mapping of parameters that gets passed as kwargs to initialize your model handler class.\n",
        "\n",
        "Default\n",
        "We provide a model handler that is built into the webserver by default. This model handler is used when no model handler is specified in the deployment input yaml. It loads a model from a checkpoint file (expected to be in the HuggingFace checkpoint format) and runs a forward pass on the model. It is a good starting point for most text generation or text embedding use cases.\n",
        "\n",
        "The parameters for the default model handler are as follows:\n",
        "\n",
        "Field\n",
        "\n",
        "Type\n",
        "\n",
        "Details\n",
        "\n",
        "task\n",
        "\n",
        "str\n",
        "\n",
        "Required. Determines how the forward pass is computed. Supported values are text-generation and feature-extraction\n",
        "\n",
        "model_dtype\n",
        "\n",
        "str\n",
        "\n",
        "The dtype that a Hugging Face model gets loaded as. Defaults to fp16. Note that bf16 is not supported by DeepSpeed, which our default model handler uses.\n",
        "\n",
        "autocast_dtype\n",
        "\n",
        "str\n",
        "\n",
        "The dtype that the model gets autocasted to if provided. Defaults to None\n",
        "\n",
        "model_name_or_path\n",
        "\n",
        "str\n",
        "\n",
        "The name of the HuggingFace repo of the model to load or the path of the locally downloaded HuggingFace checkpoint.\n",
        "\n",
        "Custom Model Handlers\n",
        "You may have a use case that is not covered by our default model handler (e.g. you want to deploy a vision model).\n",
        "\n",
        "If you’d like to define your own model handler, you can implement a class that exposes the below interface. Note that the format of the requests that are passed into the handler follow the exact same format as the requests you send to the webserver. See Querying a Deployment for details on the input request format of the webserver.\n",
        "\n",
        "class ModelHandlerInterface:\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        '''\n",
        "        The init function you define can have keyword arguments equal\n",
        "        to the values passed in the `model_parameters` section of the deployment YAML.\n",
        "        '''\n",
        "\n",
        "    def predict(self, model_requests: List[Dict[str, Any]]):\n",
        "        '''\n",
        "        Specify the logic of your model's forward pass.\n",
        "        For example for Hugging Face models for text generation, this would be a call to generate().\n",
        "\n",
        "        The `model_requests` is a list of dictionaries where each dictionary\n",
        "        is an individual request and the list represents a batch of requests.\n",
        "\n",
        "        Note that each dictionary in the list is guaranteed to have\n",
        "        two keys: `input` and `parameters`. These are almost the same `inputs` and `parameters`\n",
        "        that you pass to the webserver when making a request, however here the `input` key\n",
        "        represents a singular input.\n",
        "        '''\n",
        "\n",
        "    def predict_stream(self, model_request: Dict[str, Any]):\n",
        "        '''\n",
        "        Optional. If your model supports streaming, implement your model's\n",
        "        behavior for streaming outputs in this method.\n",
        "\n",
        "        `model_request` is a dictionary which has two keys: `input` and `parameters`.\n",
        "        These are almost the same `inputs` and `parameters` that you pass to the webserver\n",
        "        when making a request, however here the `input` key represents a singular input.\n",
        "        '''\n",
        "There are some examples you can follow in the examples repo here.\n",
        "\n",
        "Let’s walk through a concrete example. Here’s a very simple model handler implementation that just returns the input string as the output of the forward pass:\n",
        "\n",
        "# Saved as hello_world_handler.py\n",
        "class HelloWorldModelHandler(ModelHandlerInterface):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.print_string = kwargs.get(\"print_string\", \"hello world!\")\n",
        "\n",
        "    def predict(self, model_requests: List[Dict[str, Any]]):\n",
        "        return [self.print_string]\n",
        "Suppose my model handler is saved in a git repo with this structure:\n",
        "\n",
        "```\n",
        "hello_world/\n",
        "├── hello_world_handler.py\n",
        "└── __init__.py\n",
        "```\n",
        "And here is a sample yaml for how you can configure your deployment to use your custom model handler:\n",
        "\n",
        "name: hello-world-model\n",
        "compute:\n",
        "  gpus: 1\n",
        "  gpu_type: a100_40gb\n",
        "replicas: 1\n",
        "image: mosaicml/inference\n",
        "integrations:\n",
        "  - integration_type: git_repo\n",
        "    git_repo: hello_world\n",
        "model:\n",
        "  model_handler: hello_world.hello_world_handler.HelloWorldModelHandler\n",
        "  model_parameters:\n",
        "    print_string: \"hello world!\"\n",
        "Downloader Function\n",
        "Default\n",
        "The downloader function allows you to customize how your model checkpoint is downloaded. This is configured by the downloader field in your deployment input yaml, which expects a python path to your downloader module, and the download_parameters field which expects a key-value mapping of parameters that gets passed as kwargs to your downloader function.\n",
        "\n",
        "If you don’t provide a custom downloader, you can use the downloader that is built into the webserver, which can download checkpoint files in the HuggingFace format from either the HuggingFace hub or s3. You must provide at most one of the parameters in the following table to download_parameters.\n",
        "\n",
        "Parameter\n",
        "\n",
        "Description\n",
        "\n",
        "Default\n",
        "\n",
        "Example\n",
        "\n",
        "Output Path\n",
        "\n",
        "hf_path\n",
        "\n",
        "The name/path of the model on HuggingFace hub.\n",
        "\n",
        "None\n",
        "\n",
        "mosaicml/mpt-7b\n",
        "\n",
        "Huggingface cache directory\n",
        "\n",
        "s3_path\n",
        "\n",
        "The path to the model on s3.\n",
        "\n",
        "None\n",
        "\n",
        "s3://my-bucket/checkpoint\n",
        "\n",
        "/mosaicml/local_model\n",
        "\n",
        "gcp_path\n",
        "\n",
        "The path to the model on GCP.\n",
        "\n",
        "None\n",
        "\n",
        "gs://my-bucket/checkpoint\n",
        "\n",
        "/mosaicml/local_model\n",
        "\n",
        "Custom Downloader\n",
        "If you’d like to download your checkpoint from a custom location, you can implement a function with the following interface where my_custom_location is passed in under the download_parameters field in your deployment YAML:\n",
        "\n",
        "def download_model(my_custom_location: str) -> None:\n",
        "    print(\"My custom location:\", my_custom_location)\n",
        "You can also take a look at this diffusion example for reference here.\n",
        "\n",
        "Again, let’s walk through a concrete example and add to the custom repo in the earlier model handler example by saving the download function to custom_downloader.py:\n",
        "\n",
        "```\n",
        "hello_world/\n",
        "├── hello_world_handler.py\n",
        "├── custom_downloader.py\n",
        "└── __init__.py\n",
        "```\n",
        "Let’s hook up the downloader to the input yaml:\n",
        "\n",
        "name: hello-world-model\n",
        "compute:\n",
        "  gpus: 1\n",
        "  gpu_type: a100_40gb\n",
        "replicas: 1\n",
        "image: mosaicml/inference\n",
        "integrations:\n",
        "  - integration_type: git_repo\n",
        "    git_repo: hello_world\n",
        "model:\n",
        "  downloader: hello_world.custom_downloader.download_model\n",
        "  download_parameters:\n",
        "    my_custom_location: my_custom_location\n",
        "  model_handler: hello_world.hello_world_handler.HelloWorldModelHandler\n",
        "  model_parameters:\n",
        "    print_string: hello world!\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "R7WKZyxtpUPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_name_and_description = \"MosaicML, a platform that makes it easier to train and fine-tune large AI models\"\n",
        "temperature = .7\n",
        "number_of_examples_per_doc = 10"
      ],
      "metadata": {
        "id": "0-1RvQpy-Hz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this to generate the dataset."
      ],
      "metadata": {
        "id": "1snNou5PrIci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "def generate_example(service_name_and_description, doc, prev_examples, temperature=.5):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"You are generating data which will be used to train a machine learning model.\n",
        "\n",
        "Specifically, you will be creating Q/A data to train a model to answer questions about a given service.\n",
        "\n",
        "You will be given a high-level description of a service, as well as a page of documentation from that service, and from that, you will generate data samples, each with a prompt/response pair. The prompt will be a question, the response will be the answer to that question.\n",
        "\n",
        "You will do so in this format:\n",
        "```\n",
        "prompt\n",
        "-----------\n",
        "$prompt_goes_here\n",
        "-----------\n",
        "\n",
        "response\n",
        "-----------\n",
        "$response_goes_here\n",
        "-----------\n",
        "```\n",
        "\n",
        "Only one prompt/response pair should be generated per turn.\n",
        "\n",
        "For each turn, make the example slightly more complex than the last, while ensuring diversity.\n",
        "\n",
        "Make sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model. Make sure each sample covers a different aspect of the doc you are looking at. This is essential.\n",
        "\n",
        "Here is the service you will be generating data about: `{service_name_and_description}`\n",
        "\n",
        "Here is the document you will generate Q/A samples from. Make sure the samples are completely based on this doc, with no outside information:\n",
        "```\n",
        "{doc}\n",
        "```\n",
        "\n",
        "Okay, now get started generating samples. Remember to keep quality and sample diversity in mind, and make each one unique.\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 10:\n",
        "            prev_examples = random.sample(prev_examples, 10)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1354,\n",
        "    )\n",
        "\n",
        "    return [choice.message['content'] for choice in response.choices]\n",
        "\n",
        "def generate_examples_for_doc(service_name_and_description, doc, number_of_examples_per_doc=50, temperature=.5):\n",
        "    examples = []\n",
        "    while len(examples) < number_of_examples_per_doc:\n",
        "        new_examples = generate_example(service_name_and_description, doc, examples, temperature)\n",
        "        for new_example in new_examples:\n",
        "            if new_example not in examples:\n",
        "                examples.append(new_example)\n",
        "\n",
        "        # Add a counter for overall examples\n",
        "        if len(examples) % 8 == 0:\n",
        "            print(f\"Generated {len(examples)} examples so far. Pausing for 20 seconds.\")\n",
        "            time.sleep(61)\n",
        "    return examples\n",
        "\n",
        "i = 0\n",
        "examples = []\n",
        "for doc in docs:\n",
        "  time.sleep(61)\n",
        "  i = i + 1\n",
        "  print(f\"Generating doc {i}/{len(docs)}'s examples\")\n",
        "  new_examples = generate_examples_for_doc(service_name_and_description, doc, number_of_examples_per_doc=number_of_examples_per_doc, temperature=.5)\n",
        "  for example in new_examples:\n",
        "    examples.append(example)\n",
        "\n",
        "  # Add a counter for overall examples\n",
        "  if len(examples) % 20 == 0:\n",
        "    print(f\"Generated {len(examples)} examples so far. Pausing for 20 seconds.\")\n",
        "    time.sleep(61)"
      ],
      "metadata": {
        "id": "tZGJsgUQWAVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to generate a system message."
      ],
      "metadata": {
        "id": "KC6iJzXjugJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_system_message(service_name_and_description):\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "          {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": f'Answer questions about this service: `{service_name_and_description.strip()}`',\n",
        "          }\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "system_message = generate_system_message(service_name_and_description)\n",
        "\n",
        "print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"
      ],
      "metadata": {
        "id": "xMcfhW6Guh2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's put our examples into a dataframe and turn them into a final pair of datasets."
      ],
      "metadata": {
        "id": "G6BqZ-hjseBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize lists to store prompts and responses\n",
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "# Parse out prompts and responses from examples\n",
        "for example in examples:\n",
        "  try:\n",
        "    split_example = example.split('-----------')\n",
        "    prompts.append(split_example[1].strip())\n",
        "    responses.append(split_example[3].strip())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'prompt': prompts,\n",
        "    'response': responses\n",
        "})\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7CEdkYeRsdmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train and test sets."
      ],
      "metadata": {
        "id": "A-8dt5qqtpgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets, with 90% in the train set\n",
        "train_df = df.sample(frac=0.9, random_state=42)\n",
        "test_df = df.drop(train_df.index)\n",
        "\n",
        "# Save the dataframes to .jsonl files\n",
        "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
        "test_df.to_json('test.jsonl', orient='records', lines=True)"
      ],
      "metadata": {
        "id": "GFPEn1omtrXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries"
      ],
      "metadata": {
        "id": "AbrFgrhG_xYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "lPG7wEPetFx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Hyperparameters"
      ],
      "metadata": {
        "id": "moVo0led-6tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"NousResearch/llama-2-7b-hf\" # use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", though keep in mind you'll need to pass a Hugging Face key argument\n",
        "dataset_name = \"/content/train.jsonl\"\n",
        "new_model = \"llama-2-7b-custom\"\n",
        "lora_r = 64\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 1\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"constant\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 25\n",
        "logging_steps = 5\n",
        "max_seq_length = None\n",
        "packing = False\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "bqfbhUZI-4c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Datasets and Train"
      ],
      "metadata": {
        "id": "F-J5p5KS_MZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_dataset = load_dataset('json', data_files='/content/train.jsonl', split=\"train\")\n",
        "valid_dataset = load_dataset('json', data_files='/content/test.jsonl', split=\"train\")\n",
        "\n",
        "# Preprocess datasets\n",
        "train_dataset_mapped = train_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n",
        "valid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n",
        "\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"all\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=5  # Evaluate every 20 steps\n",
        ")\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_mapped,\n",
        "    eval_dataset=valid_dataset_mapped,  # Pass validation dataset here\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(new_model)\n",
        "\n",
        "# Cell 4: Test the model\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nHow do I create a new run? [/INST]\" # replace the command here with something relevant to your task\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'].replace(prompt, '').split('<</response>>')[0])"
      ],
      "metadata": {
        "id": "qf1qxbiF-x6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Inference"
      ],
      "metadata": {
        "id": "F6fux9om_c4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWhat might a typical training YAML look like?/INST]\" # replace the command here with something relevant to your task\n",
        "num_new_tokens = 250  # change to the number of new tokens you want to generate\n",
        "\n",
        "# Count the number of tokens in the prompt\n",
        "num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
        "\n",
        "# Calculate the maximum length for the generation\n",
        "max_length = num_prompt_tokens + num_new_tokens\n",
        "\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n",
        "result = gen(prompt)\n",
        "print(result[0]['generated_text'].replace(prompt, '').split('<</response>>')[0])"
      ],
      "metadata": {
        "id": "7hxQ_Ero2IJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge the model and store in Google Drive"
      ],
      "metadata": {
        "id": "Ko6UkINu_qSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and save the fine-tuned model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n",
        "\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Save the merged model\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "AgKCL7fTyp9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a fine-tuned model from Drive and run inference"
      ],
      "metadata": {
        "id": "do-dFdE5zWGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "xg6nHPsLzMw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "prompt = \"What is 2 + 2?\"  # change to your desired prompt\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "result = gen(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "fBK2aE2KzZ05"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOSyx81WouUbbg26dXCmTxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}